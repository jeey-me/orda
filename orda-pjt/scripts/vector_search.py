#!/usr/bin/env python3
"""
ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ - Pinecone í†µí•© ê´€ë¦¬
ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë²¡í„° ê²€ìƒ‰ ë° ê´€ë¦¬ë¥¼ ë‹´ë‹¹
"""

import os
import asyncio
import json
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import traceback
from pathlib import Path

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec

class VectorSearchEngine:
    """Pinecone ë²¡í„° ê²€ìƒ‰ ë° ê´€ë¦¬ ì—”ì§„"""
    
    def __init__(self, index_name: str = "lastproject"):
        """
        ì´ˆê¸°í™”
        
        Args:
            index_name: Pinecone ì¸ë±ìŠ¤ ì´ë¦„
        """
        load_dotenv(override=True)
        
        self.index_name = index_name
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        
        if not self.openai_api_key or not self.pinecone_api_key:
            raise ValueError("OPENAI_API_KEY ë˜ëŠ” PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key
        )
        
        # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=self.pinecone_api_key)
        
        # ì¸ë±ìŠ¤ ì—°ê²°
        self._connect_index()
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self._init_vector_stores()
    
    def _connect_index(self):
        """Pinecone ì¸ë±ìŠ¤ ì—°ê²°"""
        try:
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
            indexes = [idx.name for idx in self.pc.list_indexes()]
            
            if self.index_name not in indexes:
                print(f"âš ï¸ ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìë™ ìƒì„± ì‹œë„)")
                self._create_index()
            
            self.index = self.pc.Index(self.index_name)
            print(f"âœ… Pinecone ì¸ë±ìŠ¤ ì—°ê²° ì™„ë£Œ: {self.index_name}")
            
        except Exception as e:
            print(f"âŒ Pinecone ì¸ë±ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _create_index(self):
        """ì¸ë±ìŠ¤ ìë™ ìƒì„±"""
        try:
            self.pc.create_index(
                name=self.index_name,
                dimension=1536,  # text-embedding-3-small ì°¨ì›
                metric="cosine",
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )
            print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
            
            # ì¸ë±ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸°
            import time
            time.sleep(30)
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _init_vector_stores(self):
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        self.namespaces = {
            "industry": "ì‚°ì—…ë¶„ë¥˜ ì •ë³´",
            "past_issue": "ê³¼ê±° ì´ìŠˆ ì •ë³´", 
            "current_issue": "í˜„ì¬ ì´ìŠˆ ì •ë³´"
        }
        
        self.vector_stores = {}
        
        for namespace, description in self.namespaces.items():
            try:
                self.vector_stores[namespace] = PineconeVectorStore(
                    index_name=self.index_name,
                    embedding=self.embedding,
                    namespace=namespace
                )
                print(f"âœ… {namespace} ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ ({description})")
                
            except Exception as e:
                print(f"âš ï¸ {namespace} ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def search_similar_past_issues(
        self, 
        query: str, 
        top_k: int = 3,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ì–´ (í˜„ì¬ ë‰´ìŠ¤ ë‚´ìš©)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ìœ ì‚¬ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ” ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰: '{query[:50]}...'")
            
            if "past_issue" not in self.vector_stores:
                print("âš ï¸ past_issue ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰
            docs = self.vector_stores["past_issue"].similarity_search_with_score(
                query, k=top_k
            )
            
            results = []
            for doc, score in docs:
                # ìœ ì‚¬ë„ ì„ê³„ê°’ í™•ì¸ (scoreê°€ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
                similarity = 1 - score  # cosine similarityë¡œ ë³€í™˜
                
                if similarity >= similarity_threshold:
                    result = {
                        "document": doc,
                        "similarity_score": similarity,
                        "parsed_content": self._parse_past_issue_document(doc),
                        "metadata": doc.metadata
                    }
                    results.append(result)
            
            print(f"âœ… {len(results)}ê°œ ìœ ì‚¬ ì´ìŠˆ ë°œê²¬ (ì„ê³„ê°’: {similarity_threshold})")
            return results
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return []
    
    async def search_related_industries(
        self,
        query: str,
        top_k: int = 3,
        similarity_threshold: float = 0.6
    ) -> List[Dict[str, Any]]:
        """
        ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ì–´
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜  
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ê´€ë ¨ ì‚°ì—… ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ­ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰: '{query[:50]}...'")
            
            if "industry" not in self.vector_stores:
                print("âš ï¸ industry ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰
            docs = self.vector_stores["industry"].similarity_search_with_score(
                query, k=top_k
            )
            
            results = []
            seen_industries = set()  # ì¤‘ë³µ ì‚°ì—… ì œê±°ìš©
            
            for doc, score in docs:
                similarity = 1 - score
                
                if similarity >= similarity_threshold:
                    parsed = self._parse_industry_document(doc)
                    industry_name = parsed.get("industry_name", "ë¯¸í™•ì¸")
                    
                    # ì¤‘ë³µ ì‚°ì—… ì œê±°
                    if industry_name not in seen_industries:
                        seen_industries.add(industry_name)
                        
                        result = {
                            "document": doc,
                            "similarity_score": similarity,
                            "parsed_content": parsed,
                            "metadata": doc.metadata
                        }
                        results.append(result)
            
            print(f"âœ… {len(results)}ê°œ ê´€ë ¨ ì‚°ì—… ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return []
    
    async def search_current_issues(
        self,
        query: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        í˜„ì¬ ì´ìŠˆ ê²€ìƒ‰ (í¬ë¡¤ë§ëœ ìµœì‹  ë°ì´í„°ì—ì„œ)
        
        Args:
            query: ê²€ìƒ‰ì–´
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            í˜„ì¬ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“° í˜„ì¬ ì´ìŠˆ ê²€ìƒ‰: '{query[:50]}...'")
            
            if "current_issue" not in self.vector_stores:
                print("âš ï¸ current_issue ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜.")
                return []
            
            docs = self.vector_stores["current_issue"].similarity_search_with_score(
                query, k=top_k
            )
            
            results = []
            for doc, score in docs:
                result = {
                    "document": doc,
                    "similarity_score": 1 - score,
                    "parsed_content": self._parse_current_issue_document(doc),
                    "metadata": doc.metadata
                }
                results.append(result)
            
            print(f"âœ… {len(results)}ê°œ í˜„ì¬ ì´ìŠˆ ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ í˜„ì¬ ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_past_issue_document(self, doc: Document) -> Dict[str, str]:
        """ê³¼ê±° ì´ìŠˆ ë¬¸ì„œ íŒŒì‹±"""
        content_lines = doc.page_content.split('\n')
        
        parsed = {
            "issue_id": "ë¯¸í™•ì¸",
            "issue_name": "ë¯¸í™•ì¸",
            "contents": "ë¯¸í™•ì¸",
            "related_industries": "ë¯¸í™•ì¸",
            "industry_reason": "ë¯¸í™•ì¸",
            "start_date": "ë¯¸í™•ì¸",
            "end_date": "ë¯¸í™•ì¸",
            "source": "ë¯¸í™•ì¸"
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('ID:') or line.startswith('ï»¿ID:'):
                parsed["issue_id"] = line.replace('ID:', '').replace('ï»¿ID:', '').strip()
            elif line.startswith('Issue_name:'):
                parsed["issue_name"] = line.replace('Issue_name:', '').strip()
            elif line.startswith('Contents:'):
                parsed["contents"] = line.replace('Contents:', '').strip()
            elif line.startswith('ê´€ë ¨ ì‚°ì—…:'):
                parsed["related_industries"] = line.replace('ê´€ë ¨ ì‚°ì—…:', '').strip()
            elif line.startswith('ì‚°ì—… ì´ìœ :'):
                parsed["industry_reason"] = line.replace('ì‚°ì—… ì´ìœ :', '').strip()
            elif line.startswith('Start_date:'):
                parsed["start_date"] = line.replace('Start_date:', '').strip()
            elif line.startswith('Fin_date:'):
                parsed["end_date"] = line.replace('Fin_date:', '').strip()
            elif line.startswith('ê·¼ê±°ìë£Œ:'):
                parsed["source"] = line.replace('ê·¼ê±°ìë£Œ:', '').strip()
        
        return parsed
    
    def _parse_industry_document(self, doc: Document) -> Dict[str, str]:
        """ì‚°ì—… ë¶„ë¥˜ ë¬¸ì„œ íŒŒì‹±"""
        content_lines = doc.page_content.split('\n')
        
        parsed = {
            "industry_name": "ë¯¸í™•ì¸",
            "description": "",
            "full_content": doc.page_content
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('KRX ì—…ì¢…ëª…:') or line.startswith('ï»¿KRX ì—…ì¢…ëª…:'):
                parsed["industry_name"] = line.replace('KRX ì—…ì¢…ëª…:', '').replace('ï»¿KRX ì—…ì¢…ëª…:', '').strip()
            elif line.startswith('ìƒì„¸ë‚´ìš©:'):
                parsed["description"] = line.replace('ìƒì„¸ë‚´ìš©:', '').strip()
                break  # ìƒì„¸ë‚´ìš©ì€ ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©
        
        return parsed
    
    def _parse_current_issue_document(self, doc: Document) -> Dict[str, str]:
        """í˜„ì¬ ì´ìŠˆ ë¬¸ì„œ íŒŒì‹±"""
        content_lines = doc.page_content.split('\n')
        
        parsed = {
            "title": "ë¯¸í™•ì¸",
            "content": "ë¯¸í™•ì¸",
            "issue_number": "ë¯¸í™•ì¸"
        }
        
        # ê°„ë‹¨í•œ íŒŒì‹± (BigKinds í¬ë¡¤ë§ ê²°ê³¼ í˜•íƒœ)
        full_content = doc.page_content
        if '\n' in full_content:
            lines = full_content.split('\n')
            if len(lines) >= 2:
                parsed["title"] = lines[0]
                parsed["content"] = '\n'.join(lines[1:])
        
        return parsed
    
    async def update_current_issues(self, issues_data: List[Dict]) -> bool:
        """
        í˜„ì¬ ì´ìŠˆ ë²¡í„° ì—…ë°ì´íŠ¸ (BigKinds í¬ë¡¤ë§ ê²°ê³¼)
        
        Args:
            issues_data: í¬ë¡¤ë§ëœ ì´ìŠˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"ğŸ”„ í˜„ì¬ ì´ìŠˆ ë²¡í„° ì—…ë°ì´íŠ¸ ì‹œì‘: {len(issues_data)}ê°œ")
            
            # ê¸°ì¡´ current_issue ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚­ì œ
            try:
                self.index.delete(delete_all=True, namespace="current_issue")
                print("âœ… ê¸°ì¡´ current_issue ë²¡í„° ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            
            # Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for issue in issues_data:
                content = f"{issue.get('ì œëª©', '')}\n{issue.get('ë‚´ìš©', '')}"
                metadata = {
                    "issue_id": issue.get('ì´ìŠˆë²ˆí˜¸', 0),
                    "title": issue.get('ì œëª©', ''),
                    "source": "bigkinds_crawling",
                    "crawled_at": datetime.now().isoformat()
                }
                
                doc = Document(page_content=content, metadata=metadata)
                documents.append(doc)
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=450,
                chunk_overlap=45,
                length_function=len
            )
            
            chunks = text_splitter.split_documents(documents)
            print(f"ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
            
            # Pineconeì— ì—…ë¡œë“œ
            if chunks:
                self.vector_stores["current_issue"] = PineconeVectorStore.from_documents(
                    chunks,
                    embedding=self.embedding,
                    index_name=self.index_name,
                    namespace="current_issue"
                )
                print("âœ… í˜„ì¬ ì´ìŠˆ ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                return True
            else:
                print("âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
        except Exception as e:
            print(f"âŒ í˜„ì¬ ì´ìŠˆ ë²¡í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            stats = self.index.describe_index_stats()
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ì •ë³´ ì •ë¦¬
            namespace_info = {}
            namespaces = stats.get('namespaces', {})
            
            for ns_name, description in self.namespaces.items():
                ns_stats = namespaces.get(ns_name, {})
                namespace_info[ns_name] = {
                    "description": description,
                    "vector_count": ns_stats.get('vector_count', 0),
                    "status": "active" if ns_stats.get('vector_count', 0) > 0 else "empty"
                }
            
            return {
                "index_name": self.index_name,
                "total_vectors": stats.get('total_vector_count', 0),
                "dimension": stats.get('dimension', 1536),
                "namespaces": namespace_info,
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def health_check(self) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ìƒíƒœ ì²´í¬"""
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "components": {}
        }
        
        try:
            # 1. Pinecone ì—°ê²° ìƒíƒœ
            try:
                stats = self.index.describe_index_stats()
                health_status["components"]["pinecone"] = {
                    "status": "healthy",
                    "total_vectors": stats.get('total_vector_count', 0)
                }
            except Exception as e:
                health_status["components"]["pinecone"] = {
                    "status": "unhealthy",
                    "error": str(e)
                }
                health_status["overall_status"] = "degraded"
            
            # 2. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ìƒíƒœ
            for namespace in self.namespaces.keys():
                try:
                    # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                    test_results = await asyncio.to_thread(
                        self.vector_stores[namespace].similarity_search,
                        "test query", k=1
                    )
                    
                    health_status["components"][f"namespace_{namespace}"] = {
                        "status": "healthy",
                        "searchable": len(test_results) >= 0
                    }
                except Exception as e:
                    health_status["components"][f"namespace_{namespace}"] = {
                        "status": "unhealthy", 
                        "error": str(e)
                    }
                    health_status["overall_status"] = "degraded"
            
            return health_status
            
        except Exception as e:
            return {
                "timestamp": datetime.now().isoformat(),
                "overall_status": "unhealthy",
                "error": str(e)
            }

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

async def batch_search(
    engine: VectorSearchEngine,
    queries: List[str],
    search_type: str = "past_issue"
) -> List[List[Dict]]:
    """
    ë°°ì¹˜ ê²€ìƒ‰ (ì—¬ëŸ¬ ì¿¼ë¦¬ ë™ì‹œ ì²˜ë¦¬)
    
    Args:
        engine: VectorSearchEngine ì¸ìŠ¤í„´ìŠ¤
        queries: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
        search_type: ê²€ìƒ‰ íƒ€ì… ("past_issue", "industry", "current_issue")
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
    """
    search_functions = {
        "past_issue": engine.search_similar_past_issues,
        "industry": engine.search_related_industries,
        "current_issue": engine.search_current_issues
    }
    
    if search_type not in search_functions:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ íƒ€ì…: {search_type}")
    
    search_func = search_functions[search_type]
    
    # ë¹„ë™ê¸° ë°°ì¹˜ ì‹¤í–‰
    tasks = [search_func(query) for query in queries]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ì˜ˆì™¸ ì²˜ë¦¬
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"âš ï¸ ì¿¼ë¦¬ {i+1} ê²€ìƒ‰ ì‹¤íŒ¨: {result}")
            processed_results.append([])
        else:
            processed_results.append(result)
    
    return processed_results

# ===== í…ŒìŠ¤íŠ¸ ë° ì§ì ‘ ì‹¤í–‰ìš© =====

async def test_vector_search():
    """ë²¡í„° ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë²¡í„° ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸")
    
    try:
        # ì—”ì§„ ì´ˆê¸°í™”
        engine = VectorSearchEngine()
        
        # 1. ìƒíƒœ ì²´í¬
        print("\n1ï¸âƒ£ ìƒíƒœ ì²´í¬")
        health = await engine.health_check()
        print(f"ì „ì²´ ìƒíƒœ: {health['overall_status']}")
        
        # 2. ì¸ë±ìŠ¤ í†µê³„
        print("\n2ï¸âƒ£ ì¸ë±ìŠ¤ í†µê³„")
        stats = engine.get_index_stats()
        print(f"ì´ ë²¡í„° ìˆ˜: {stats.get('total_vectors', 0)}")
        for ns, info in stats.get('namespaces', {}).items():
            print(f"  â€¢ {ns}: {info['vector_count']}ê°œ ({info['status']})")
        
        # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_query = "SKí…”ë ˆì½¤ ê³ ê° ì´íƒˆ ë³´ì•ˆ ì‚¬ê³ "
        
        print(f"\n3ï¸âƒ£ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{test_query}'")
        
        # ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰
        past_issues = await engine.search_similar_past_issues(test_query, top_k=2)
        print(f"ê³¼ê±° ì´ìŠˆ: {len(past_issues)}ê°œ ë°œê²¬")
        for issue in past_issues:
            parsed = issue["parsed_content"]
            print(f"  â€¢ {parsed['issue_name']} (ìœ ì‚¬ë„: {issue['similarity_score']:.2f})")
        
        # ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰
        industries = await engine.search_related_industries(test_query, top_k=2)
        print(f"ê´€ë ¨ ì‚°ì—…: {len(industries)}ê°œ ë°œê²¬")
        for industry in industries:
            parsed = industry["parsed_content"]
            print(f"  â€¢ {parsed['industry_name']} (ìœ ì‚¬ë„: {industry['similarity_score']:.2f})")
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_vector_search())
    def initialize(self):
        """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”ìš© placeholder"""
        print("âœ… VectorSearchEngine initialized (placeholder)")

    def close(self):
        """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬ìš© placeholder"""
        print("ğŸ”’ VectorSearchEngine closed (placeholder)")
