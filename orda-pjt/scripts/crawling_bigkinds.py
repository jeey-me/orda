#!/usr/bin/env python3
"""
BigKinds Crawler - Enhanced Multi-Category Version
7ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° 10ê°œì”© ì´ 70ê°œ ì´ìŠˆ í¬ë¡¤ë§ + ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time
import traceback
import json
from datetime import datetime
import os
from pathlib import Path
from typing import List, Dict, Optional

class BigKindsCrawler:
    """BigKinds ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ëŸ¬"""
    
    # í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ (ì „ì²´ ì œì™¸)
    TARGET_CATEGORIES = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”", "êµ­ì œ", "ì§€ì—­", "ITê³¼í•™"]
    
    def __init__(self, data_dir: str = "data2", headless: bool = False, issues_per_category: int = 10):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‹¤í–‰ ì—¬ë¶€
            issues_per_category: ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§í•  ì´ìŠˆ ìˆ˜
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.headless = headless
        self.issues_per_category = issues_per_category
        self.driver = None
        self.wait = None
        
        # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
        self.crawling_results = {
            "total_issues": 0,
            "categories": {},
            "crawling_log": [],
            "crawled_at": "",
            "all_issues": [],  
        }
        
        # í¬ë¡¬ ì˜µì…˜ ì„¤ì •
        self.chrome_options = webdriver.ChromeOptions()
        self.chrome_options.add_argument('--headless')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        

    def crawl_current_issues(self, category: str = "ì „ì²´", max_issues: int = 10) -> Dict:
        """
        ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ
        
        Args:
            category: í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ("ì „ì²´"ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§)
            max_issues: í¬ë¡¤ë§í•  ìµœëŒ€ ì´ìŠˆ ìˆ˜
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if category == "ì „ì²´":
            # ìƒˆë¡œìš´ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰
            return self.crawl_all_categories()
        else:
            # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹ê³¼ ìœ ì‚¬)
            return self._crawl_single_category(category, max_issues)

    def crawl_all_categories(self) -> Dict:
        """
        ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ì´ìŠˆ í¬ë¡¤ë§
        
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = datetime.now()
        self.crawling_results["crawled_at"] = start_time.strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"ğŸš€ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“‹ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: {', '.join(self.TARGET_CATEGORIES)}")
        print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì´ìŠˆ ìˆ˜: {self.issues_per_category}ê°œ")
        print(f"ğŸ¯ ì˜ˆìƒ ì´ ì´ìŠˆ ìˆ˜: {len(self.TARGET_CATEGORIES) * self.issues_per_category}ê°œ")
        
        try:
            self._setup_driver()
            self._navigate_to_bigkinds()
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
            for idx, category in enumerate(self.TARGET_CATEGORIES, 1):
                print(f"\n{'='*60}")
                print(f"ğŸ“‚ [{idx}/{len(self.TARGET_CATEGORIES)}] '{category}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
                print(f"{'='*60}")
                
                try:
                    category_issues = self._crawl_category(category)
                    self.crawling_results["categories"][category] = category_issues
                    self.crawling_results["all_issues"].extend(category_issues)
                    
                    print(f"âœ… '{category}' ì¹´í…Œê³ ë¦¬ ì™„ë£Œ: {len(category_issues)}ê°œ ì´ìŠˆ")
                    self.crawling_results["crawling_log"].append({
                        "category": category,
                        "status": "success",
                        "issues_count": len(category_issues),
                        "timestamp": datetime.now().strftime("%H:%M:%S")
                    })
                    
                    # ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸° (ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€)
                    if idx < len(self.TARGET_CATEGORIES):
                        print("â³ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ëŒ€ê¸° ì¤‘... (3ì´ˆ)")
                        time.sleep(3)
                    
                except Exception as e:
                    error_msg = f"âŒ '{category}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"
                    print(error_msg)
                    self.crawling_results["crawling_log"].append({
                        "category": category,
                        "status": "failed",
                        "error": str(e),
                        "timestamp": datetime.now().strftime("%H:%M:%S")
                    })
                    # í•œ ì¹´í…Œê³ ë¦¬ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ê³„ì† ì§„í–‰
                    continue
            
            # ê²°ê³¼ ì •ë¦¬
            self.crawling_results["total_issues"] = len(self.crawling_results["all_issues"])
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self._save_results()
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            self._print_summary()
            
            return {
                **self.crawling_results,
                "saved_file": saved_file,
                "execution_time": str(datetime.now() - start_time)
            }
            
        except Exception as e:
            print(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            raise
        finally:
            self._cleanup_driver()

    def _crawl_single_category(self, category: str, max_issues: int) -> Dict:
        """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹ í˜¸í™˜)"""
        print(f"ğŸ¯ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§: {category}")
        
        try:
            self._setup_driver()
            self._navigate_to_bigkinds()
            
            category_issues = self._crawl_category(category, max_issues)
            
            result = {
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_issues": len(category_issues),
                "source": "bigkinds.or.kr",
                "category": category,
                "issues": category_issues
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
        finally:
            self._cleanup_driver()

    def _setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • - ìë™í™” íƒì§€ ë°©ì§€ ê¸°ëŠ¥ í¬í•¨"""
        options = webdriver.ChromeOptions()
        
        if self.headless:
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
        else:
            options.add_argument("--start-maximized")
        
        # ìë™í™” íƒì§€ ë°©ì§€
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # ì¶”ê°€ ì•ˆì •ì„± ì„¤ì •
        options.add_argument("--disable-web-security")
        options.add_argument("--allow-running-insecure-content")

        self.driver = webdriver.Chrome(options=options)
        # webdriver ì†ì„± ìˆ¨ê¸°ê¸°
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.wait = WebDriverWait(self.driver, 15)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")

    def _cleanup_driver(self):
        """ë“œë¼ì´ë²„ ì•ˆì „ ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
                print("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.driver = None
                self.wait = None

    def _navigate_to_bigkinds(self):
        """BigKinds ì‚¬ì´íŠ¸ ì ‘ì† ë° ì´ˆê¸° ì„¤ì •"""
        print("ğŸŒ BigKinds ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
        
        try:
            self.driver.get("https://www.bigkinds.or.kr/")
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # íŒì—… ë‹«ê¸° (ìˆë‹¤ë©´)
            try:
                popup_close = self.driver.find_element(By.CSS_SELECTOR, ".popup-close-btn")
                popup_close.click()
            except:
                pass
                
            print("âœ… BigKinds ì‚¬ì´íŠ¸ ì ‘ì† ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {str(e)}")
            return False

    def _scroll_to_issues_section(self):
        """ì˜¤ëŠ˜ì˜ ì´ìŠˆ ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤"""
        try:
            self.driver.execute_script("window.scrollTo(0, 880);")
            time.sleep(2)
            print("âœ… ì´ìŠˆ ì„¹ì…˜ ìŠ¤í¬ë¡¤ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë¡¤ ì‹¤íŒ¨: {e}")

    def _crawl_category(self, category: str, max_issues: int = None) -> List[Dict]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì´ìŠˆë“¤ í¬ë¡¤ë§
        
        Args:
            category: í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ëª…
            max_issues: ìµœëŒ€ ì´ìŠˆ ìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        if max_issues is None:
            max_issues = self.issues_per_category
            
        try:
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ ë° ìŠ¤í¬ë¡¤
            self._scroll_to_issues_section() 
            self._click_category(category)
            
            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì´ìŠˆë“¤ í¬ë¡¤ë§
            issues = self._crawl_issues_in_category(category, max_issues)
            
            return issues
            
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise

    def _click_category(self, category: str):
        """ì¹´í…Œê³ ë¦¬ ì„ íƒ"""
        try:
            print(f"ğŸ¯ '{category}' ì¹´í…Œê³ ë¦¬ ì„ íƒ ì¤‘...")
            
            # ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            category_selector = f'a.issue-category[data-category="{category}"]'
            category_button = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, category_selector))
            )
            
            # JavaScriptë¡œ í´ë¦­ (ë” ì•ˆì •ì )
            self.driver.execute_script("arguments[0].click();", category_button)
            time.sleep(4)  # ì¹´í…Œê³ ë¦¬ ë³€ê²½ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            
            print(f"âœ… '{category}' ì¹´í…Œê³ ë¦¬ ì„ íƒ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ '{category}' ì„ íƒ ì‹¤íŒ¨: {e}")
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™”, êµ­ì œ, ì§€ì—­, ITê³¼í•™")
            raise

    def _select_category(self, category: str) -> bool:
        try:
            # ì¹´í…Œê³ ë¦¬ ë©”ë‰´ í´ë¦­
            menu_btn = self.driver.find_element(By.CSS_SELECTOR, ".category-menu-btn")
            menu_btn.click()
            time.sleep(1)
            
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ
            category_btn = self.driver.find_element(By.XPATH, 
                f"//div[contains(@class, 'category-item') and contains(text(), '{category}')]")
            category_btn.click()
            time.sleep(2)
            
            print(f"âœ… '{category}' ì¹´í…Œê³ ë¦¬ ì„ íƒ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ '{category}' ì„ íƒ ì‹¤íŒ¨: {str(e)}")
            print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {', '.join(self.TARGET_CATEGORIES)}")
            return False

    def _crawl_issues_in_category(self, category: str, max_issues: int) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ ë‚´ ì´ìŠˆë“¤ í¬ë¡¤ë§"""
        issues = []
        
        for i in range(1, max_issues + 1):
            print(f"  ğŸ“° [{i}/{max_issues}] ì´ìŠˆ ì²˜ë¦¬ ì¤‘...")
            
            try:
                # 3ë²ˆì§¸ ì´ìŠˆë¶€í„°ëŠ” ìŠ¬ë¼ì´ë“œ ë„˜ê¸°ê¸° í•„ìš”
                if i >= 3:
                    self._navigate_slides(i)
                
                # ì´ìŠˆ ë°ì´í„° ì¶”ì¶œ
                issue_data = self._extract_issue_data(i, category)
                print(f"ğŸ“Œ DEBUG: crawling_results keys: {list(self.crawling_results.keys())}")
                if issue_data:
                    issues.append(issue_data)
                    print(f"    âœ… ì´ìŠˆ {i} ì¶”ì¶œ ì™„ë£Œ: {issue_data['ì œëª©'][:30]}...")
                
                # íŒì—… ë‹«ê¸° ë° ìœ„ì¹˜ ë³µì›
                self._close_popup_and_restore()
                
                # ì´ìŠˆ ê°„ ì§§ì€ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                print(f"    âŒ ì´ìŠˆ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ê°œë³„ ì´ìŠˆ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì´ìŠˆ ê³„ì† ì§„í–‰
                continue
        
        return issues

    def _navigate_slides(self, issue_num: int):
        """ìŠ¬ë¼ì´ë“œ ë„˜ê¸°ê¸° (3ë²ˆì§¸ ì´ìŠˆë¶€í„° í•„ìš”)"""
        slides_to_move = issue_num - 3
        
        for slide in range(slides_to_move):
            try:
                next_btn = self.driver.find_element(
                    By.CSS_SELECTOR, 
                    'div.swiper-button-next.section2-btn.st2-sw1-next'
                )
                
                # ë²„íŠ¼ì´ ë¹„í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
                is_disabled = next_btn.get_attribute('aria-disabled') == 'true'
                if is_disabled:
                    print(f"    âš ï¸ ìŠ¬ë¼ì´ë“œ ëì— ë„ë‹¬ (ì´ìŠˆ {issue_num})")
                    break
                
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(1)
                
            except Exception as e:
                print(f"    âš ï¸ ìŠ¬ë¼ì´ë“œ ë„˜ê¸°ê¸° ì‹¤íŒ¨ (ì´ìŠˆ {issue_num}): {e}")
                break

    def _extract_issue_data(self, issue_num: int, category: str) -> Optional[Dict]:
        """ê°œë³„ ì´ìŠˆ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì´ìŠˆ í´ë¦­
            issue_selector = f'div.swiper-slide:nth-child({issue_num}) .issue-item-link'
            issue_element = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, issue_selector))
            )
            
            # ìš”ì†Œê°€ ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤ í›„ í´ë¦­
            self.driver.execute_script("arguments[0].scrollIntoView(true);", issue_element)
            time.sleep(0.5)
            self.driver.execute_script("arguments[0].click();", issue_element)
            
            # íŒì—… ë‚´ìš© ì¶”ì¶œ
            title_elem = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'p.issuPopTitle'))
            )
            content_elem = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'p.pT20.issuPopContent'))
            )

            title = title_elem.text.strip()
            content = content_elem.text.strip()
            
            # ìœ ë‹ˆí¬ ID ìƒì„± (ì¹´í…Œê³ ë¦¬ + ìˆœì„œ)
            unique_id = f"{category}_{issue_num}"

            return {
                "ì´ìŠˆë²ˆí˜¸": len(self.crawling_results["all_issues"]) + 1,  # ì „ì²´ ìˆœì„œ
                "ì¹´í…Œê³ ë¦¬ë³„_ë²ˆí˜¸": issue_num,  # ì¹´í…Œê³ ë¦¬ ë‚´ ìˆœì„œ
                "ì¹´í…Œê³ ë¦¬": category,
                "ì œëª©": title,
                "ë‚´ìš©": content,
                "ì¶”ì¶œì‹œê°„": datetime.now().isoformat(),
                "ê³ ìœ ID": unique_id
            }
            
        except Exception as e:
            print(f"    âŒ ì´ìŠˆ {issue_num} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            print(f"   â†³ ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            print(f"   â†³ ë©”ì‹œì§€: {str(e)}")
            traceback.print_exc()
            return None

    def _close_popup_and_restore(self):
        """íŒì—… ë‹«ê¸° ë° ìŠ¤í¬ë¡¤ ìœ„ì¹˜ ë³µì›"""
        try:
            # ESCë¡œ íŒì—… ë‹«ê¸°
            ActionChains(self.driver).send_keys(Keys.ESCAPE).perform()
            time.sleep(1)
            
            # ë‹¤ì‹œ ì´ìŠˆ ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, 880);")
            time.sleep(1)
            
        except Exception as e:
            print(f"    âš ï¸ íŒì—… ë‹«ê¸° ì‹¤íŒ¨: {e}")

    def _save_results(self) -> str:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì´ ì´ìŠˆ ìˆ˜ë¥¼ í¬í•¨í•œ íŒŒì¼ëª…
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_MultiCategory_{self.crawling_results['total_issues']}issues.json"
            filepath = self.data_dir / filename
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì¡°
            save_data = {
                **self.crawling_results,
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "crawler_version": "BigKindsCrawler_v1.0"
                }
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _print_summary(self):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"ğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   â€¢ ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬: {len(self.crawling_results['categories'])}/{len(self.TARGET_CATEGORIES)}ê°œ")
        print(f"   â€¢ ì´ ìˆ˜ì§‘ ì´ìŠˆ: {self.crawling_results['total_issues']}ê°œ")
        print(f"   â€¢ í¬ë¡¤ë§ ì‹œê°„: {self.crawling_results['crawled_at']}")
        
        print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼:")
        for category, issues in self.crawling_results["categories"].items():
            print(f"   â€¢ {category}: {len(issues)}ê°œ ì´ìŠˆ")
        
        # ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬ê°€ ìˆë‹¤ë©´ í‘œì‹œ
        failed_categories = [log for log in self.crawling_results["crawling_log"] if log["status"] == "failed"]
        if failed_categories:
            print(f"\nâš ï¸ ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬:")
            for failed in failed_categories:
                print(f"   â€¢ {failed['category']}: {failed['error']}")
        
        print(f"\nâœ… í¬ë¡¤ë§ ì„±ê³µë¥ : {len(self.crawling_results['categories'])}/{len(self.TARGET_CATEGORIES)} ({len(self.crawling_results['categories'])/len(self.TARGET_CATEGORIES)*100:.1f}%)")

    def load_latest_results(self) -> Optional[Dict]:
        """ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ ë¡œë“œ"""
        try:
            json_files = list(self.data_dir.glob("*_MultiCategory_*issues.json"))
            if not json_files:
                print("ğŸ“‚ ì €ì¥ëœ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… ìµœì‹  ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ: {latest_file.name}")
            print(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {data.get('total_issues', 0)}ê°œ ì´ìŠˆ")
            
            return data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None


# í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„±)
def crawl_all_categories_quick(headless: bool = True, issues_per_category: int = 10) -> Dict:
    """ë¹ ë¥¸ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ í•¨ìˆ˜"""
    crawler = BigKindsCrawler(headless=headless, issues_per_category=issues_per_category)
    return crawler.crawl_all_categories()

def load_latest_multi_data() -> Optional[Dict]:
    """ìµœì‹  ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ"""
    crawler = BigKindsCrawler()
    return crawler.load_latest_results()


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ BigKinds Multi-Category Crawler")
    
    # í¬ë¡¤ëŸ¬ ì„¤ì •
    crawler = BigKindsCrawler(
        headless=False,  # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ë¸Œë¼ìš°ì € ë³´ê¸°
        issues_per_category=10  # ì¹´í…Œê³ ë¦¬ë³„ 10ê°œì”©
    )
    
    try:
        # ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰
        result = crawler.crawl_all_categories()
        
        print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
        print(f"   â€¢ ì €ì¥ íŒŒì¼: {result['saved_file']}")
        print(f"   â€¢ ì‹¤í–‰ ì‹œê°„: {result['execution_time']}")
        print(f"   â€¢ ì´ ì´ìŠˆ ìˆ˜: {result['total_issues']}ê°œ")
        
        # ê²°ê³¼ ë°ì´í„° ê°„ë‹¨ í™•ì¸
        if result['all_issues']:
            print(f"\nğŸ“° ìˆ˜ì§‘ëœ ì´ìŠˆ ì˜ˆì‹œ:")
            for i, issue in enumerate(result['all_issues'][:3], 1):
                print(f"   {i}. [{issue['ì¹´í…Œê³ ë¦¬']}] {issue['ì œëª©'][:50]}...")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()