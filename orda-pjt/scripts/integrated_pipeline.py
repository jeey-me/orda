#!/usr/bin/env python3
"""
Complete Integrated Pipeline for BigKinds News Analysis with RAG
ì „ì²´ í”„ë¡œì„¸ìŠ¤: í¬ë¡¤ë§ â†’ AI í•„í„°ë§ â†’ ì‹¤ì œ RAG ë¶„ì„ â†’ API ì¤€ë¹„
"""

import os
import json
import time
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import traceback
import pandas as pd

# í™˜ê²½ ë³€ìˆ˜ ë° AI ëª¨ë¸ ì„¤ì •
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from crawling_bigkinds import BigKindsCrawler
    from stock_market_filter import StockMarketFilter
except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("í•„ìš”í•œ íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:")
    print("- crawling_bigkinds.py")
    print("- stock_market_filter.py")

class RealRAGAnalysisExecutor:
    """ì‹¤ì œ RAG ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        load_dotenv(override=True)
        
        # í™˜ê²½ ì„¤ì •
        self.EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        self.INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "lastproject")
        
        # LLM ì´ˆê¸°í™” (GPT-4o-mini ì‚¬ìš©)
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.embedding = OpenAIEmbeddings(model=self.EMBEDDING_MODEL)
        
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.industry_store = PineconeVectorStore(
            index_name=self.INDEX_NAME,
            embedding=self.embedding,
            namespace="industry"
        )
        
        self.past_issue_store = PineconeVectorStore(
            index_name=self.INDEX_NAME,
            embedding=self.embedding,
            namespace="past_issue"
        )
        

        # ì‚°ì—… DB ë¡œë”© (ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •)
        try:
            # scripts í´ë”ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ../data/ ê²½ë¡œ ì‚¬ìš©
            self.industry_df = pd.read_csv("../data/ì‚°ì—…DB.v.0.3.csv")
            self.industry_dict = dict(zip(self.industry_df["KRX ì—…ì¢…ëª…"], self.industry_df["ìƒì„¸ë‚´ìš©"]))
            self.valid_krx_names = list(self.industry_df["KRX ì—…ì¢…ëª…"].unique())
            print(f"âœ… ì‚°ì—… DB ë¡œë“œ: {len(self.valid_krx_names)}ê°œ ì—…ì¢…")
        except Exception as e:
            print(f"âš ï¸ ì‚°ì—… DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.industry_dict = {}
            self.valid_krx_names = []
        
        # ê³¼ê±° ì´ìŠˆ DB ë¡œë”© (ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •)
        try:
            # scripts í´ë”ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ../data/ ê²½ë¡œ ì‚¬ìš©
            self.past_df = pd.read_csv("../data/Past_news.csv")
            self.issue_dict = dict(zip(self.past_df["Issue_name"], self.past_df["Contents"] + "\n\nìƒì„¸: " + self.past_df["Contentes(Spec)"]))
            self.valid_issue_names = list(self.past_df["Issue_name"].unique())
            print(f"âœ… ê³¼ê±° ì´ìŠˆ DB ë¡œë“œ: {len(self.valid_issue_names)}ê°œ ì´ìŠˆ")
        except Exception as e:
            print(f"âš ï¸ ê³¼ê±° ì´ìŠˆ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.issue_dict = {}
            self.valid_issue_names = []
        
        print("âœ… ì‹¤ì œ RAG ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (GPT-4o-mini)")
    
    def analyze_industry_for_issue(self, issue: Dict) -> List[Dict]:
        """íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ê´€ë ¨ ì‚°ì—… ë¶„ì„ (ì‹¤ì œ RAG)"""
        try:
            query = f"{issue.get('ì œëª©', '')}\n{issue.get('ì›ë³¸ë‚´ìš©', issue.get('ë‚´ìš©', ''))}"
            print(f"ğŸ­ ì‚°ì—… ë¶„ì„ ì¤‘: {issue.get('ì œëª©', 'N/A')[:30]}...")
            
            # Step 1: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ
            results = self.industry_store.similarity_search_with_score(query, k=10)
            
            vector_candidates = []
            for doc, score in results:
                content = doc.page_content.replace('\ufeff', '').replace('ï»¿', '')
                
                if "KRX ì—…ì¢…ëª…:" in content:
                    lines = content.split("\n")
                    for line in lines:
                        if "KRX ì—…ì¢…ëª…:" in line:
                            industry_name = line.replace("KRX ì—…ì¢…ëª…:", "").strip()
                            if industry_name in self.industry_dict:
                                # ì¤‘ë³µ ì²´í¬
                                if not any(c["name"] == industry_name for c in vector_candidates):
                                    similarity_percentage = round((1 - score) * 100, 1)
                                    
                                    content_parts = content.split("ìƒì„¸ë‚´ìš©:")
                                    industry_detail = content_parts[1].strip() if len(content_parts) > 1 else self.industry_dict[industry_name]
                                    
                                    vector_candidates.append({
                                        "name": industry_name,
                                        "similarity": similarity_percentage,
                                        "description": industry_detail
                                    })
                            break
            
            # Step 2: AI Agentë¡œ ê´€ë ¨ ì‚°ì—… í›„ë³´ ì¶”ì¶œ
            ai_candidates = self._extract_candidate_industries(query, self.valid_krx_names, top_k=10)
            
            # Step 3: ê²°ê³¼ ê²°í•© ë° ê²€ì¦
            final_candidates = self._combine_and_validate_industry_results(
                query, vector_candidates, ai_candidates, self.industry_dict
            )
            
            print(f"   âœ… ì‚°ì—… ë¶„ì„ ì™„ë£Œ: ìƒìœ„ {min(3, len(final_candidates))}ê°œ ì„ íƒ")
            return final_candidates[:3]  # ìƒìœ„ 3ê°œ ë°˜í™˜
            
        except Exception as e:
            print(f"âŒ ì‚°ì—… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_past_issues_for_issue(self, issue: Dict) -> List[Dict]:
        """íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ë¶„ì„ (ì‹¤ì œ RAG)"""
        try:
            query = f"{issue.get('ì œëª©', '')}\n{issue.get('ì›ë³¸ë‚´ìš©', issue.get('ë‚´ìš©', ''))}"
            print(f"ğŸ“š ê³¼ê±° ì´ìŠˆ ë¶„ì„ ì¤‘: {issue.get('ì œëª©', 'N/A')[:30]}...")
            
            # Step 1: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ
            results = self.past_issue_store.similarity_search_with_score(query, k=10)
            
            vector_candidates = []
            for doc, score in results:
                content = doc.page_content.replace('\ufeff', '').replace('ï»¿', '')
                
                if "Issue_name:" in content:
                    lines = content.split("\n")
                    for line in lines:
                        if "Issue_name:" in line:
                            issue_name = line.replace("Issue_name:", "").strip()
                            if issue_name in self.issue_dict:
                                # ì¤‘ë³µ ì²´í¬
                                if not any(c["name"] == issue_name for c in vector_candidates):
                                    similarity_percentage = round((1 - score) * 100, 1)
                                    
                                    content_parts = content.split("Contents:")
                                    issue_detail = content_parts[1].strip() if len(content_parts) > 1 else self.issue_dict[issue_name]
                                    
                                    # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ
                                    period = "N/A"
                                    for line in lines:
                                        if "Start_date:" in line and "Fin_date:" in line:
                                            start = line.split("Start_date:")[1].split("Fin_date:")[0].strip()
                                            end = line.split("Fin_date:")[1].strip()
                                            period = f"{start} ~ {end}"
                                            break
                                    
                                    vector_candidates.append({
                                        "name": issue_name,
                                        "similarity": similarity_percentage,
                                        "description": issue_detail,
                                        "period": period
                                    })
                            break
            
            # Step 2: AI Agentë¡œ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ í›„ë³´ ì¶”ì¶œ
            ai_candidates = self._extract_candidate_past_issues(query, self.valid_issue_names, top_k=10)
            
            # Step 3: ê²°ê³¼ ê²°í•© ë° ê²€ì¦
            final_candidates = self._combine_and_validate_past_issue_results(
                query, vector_candidates, ai_candidates, self.issue_dict
            )
            
            print(f"   âœ… ê³¼ê±° ì´ìŠˆ ë¶„ì„ ì™„ë£Œ: ìƒìœ„ {min(3, len(final_candidates))}ê°œ ì„ íƒ")
            return final_candidates[:3]  # ìƒìœ„ 3ê°œ ë°˜í™˜
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_candidate_industries(self, news_content: str, industry_list: List[str], top_k: int = 10) -> List[Dict]:
        """AI Agentê°€ ë‰´ìŠ¤ ë‚´ìš©ì„ ë³´ê³  ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚°ì—…ë“¤ì„ ì¶”ì¶œ"""
        if not industry_list:
            return []
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” ë‰´ìŠ¤ì™€ ì‚°ì—…ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
ì£¼ì–´ì§„ ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì œê³µëœ KRX ì—…ì¢… ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚°ì—…ë“¤ì„ ì„ ë³„í•´ì•¼ í•´.

ê´€ë ¨ì„± íŒë‹¨ ê¸°ì¤€:
1. ì§ì ‘ì  ì˜í–¥: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—…ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
2. ê³µê¸‰ë§ ê´€ê³„: ë‰´ìŠ¤ ê´€ë ¨ ê¸°ì—…/ì‚°ì—…ê³¼ ê³µê¸‰ë§ ê´€ê³„ê°€ ìˆëŠ”ê°€?
3. ì‹œì¥ ë™í–¥: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—…ì˜ ì‹œì¥ ë™í–¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
4. ì •ì±…/ê·œì œ: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—… ê´€ë ¨ ì •ì±…ì´ë‚˜ ê·œì œì™€ ì—°ê´€ë˜ëŠ”ê°€?"""),
            ("human", """
[ë‰´ìŠ¤ ë‚´ìš©]
{news}

[KRX ì—…ì¢… ë¦¬ìŠ¤íŠ¸]  
{industries}

ìœ„ ë‰´ìŠ¤ì™€ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚°ì—…ì„ {top_k}ê°œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ì‚°ì—…ì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜(1-10ì )ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "candidates": [
    {{"industry": "ì‚°ì—…ëª…", "score": ì ìˆ˜, "reason": "ê´€ë ¨ì„± ì´ìœ "}},
    ...
  ]
}}""")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser
        
        try:
            result = chain.invoke({
                "news": news_content,
                "industries": ", ".join(industry_list[:50]),  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
                "top_k": top_k
            })
            return result.get("candidates", [])
        except Exception as e:
            print(f"âŒ AI ì‚°ì—… í›„ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_candidate_past_issues(self, news_content: str, issue_list: List[str], top_k: int = 10) -> List[Dict]:
        """AI Agentê°€ ë‰´ìŠ¤ ë‚´ìš©ì„ ë³´ê³  ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³¼ê±° ì´ìŠˆë“¤ì„ ì¶”ì¶œ"""
        if not issue_list:
            return []
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” í˜„ì¬ ë‰´ìŠ¤ì™€ ê³¼ê±° ì´ìŠˆì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
ì£¼ì–´ì§„ í˜„ì¬ ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì œê³µëœ ê³¼ê±° ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ìŠˆë“¤ì„ ì„ ë³„í•´ì•¼ í•´.

ê´€ë ¨ì„± íŒë‹¨ ê¸°ì¤€:
1. ìœ ì‚¬í•œ ì‹œì¥ ìƒí™©: ê³¼ê±° ì´ìŠˆì™€ í˜„ì¬ ìƒí™©ì´ ìœ ì‚¬í•œ ì‹œì¥ í™˜ê²½ì¸ê°€?
2. ë™ì¼í•œ ì‚°ì—…/ê¸°ì—… ì˜í–¥: ê°™ì€ ì‚°ì—…ì´ë‚˜ ìœ ì‚¬í•œ ê¸°ì—…ë“¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
3. ì •ì±…/ê²½ì œì  ìœ ì‚¬ì„±: ì •ì±… ë³€í™”ë‚˜ ê²½ì œì  ìš”ì¸ì´ ìœ ì‚¬í•œê°€?
4. íˆ¬ìì ì‹¬ë¦¬: íˆ¬ììë“¤ì˜ ë°˜ì‘ì´ë‚˜ ì‹œì¥ ì‹¬ë¦¬ê°€ ë¹„ìŠ·í•œê°€?"""),
            ("human", """
[í˜„ì¬ ë‰´ìŠ¤ ë‚´ìš©]
{news}

[ê³¼ê±° ì´ìŠˆ ë¦¬ìŠ¤íŠ¸]
{issues}

ìœ„ í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³¼ê±° ì´ìŠˆë¥¼ {top_k}ê°œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ê³¼ê±° ì´ìŠˆì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜(1-10ì )ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "candidates": [
    {{"issue": "ì´ìŠˆëª…", "score": ì ìˆ˜, "reason": "ê´€ë ¨ì„± ì´ìœ "}},
    ...
  ]
}}""")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser
        
        try:
            result = chain.invoke({
                "news": news_content,
                "issues": ", ".join(issue_list[:50]),  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
                "top_k": top_k
            })
            return result.get("candidates", [])
        except Exception as e:
            print(f"âŒ AI ê³¼ê±° ì´ìŠˆ í›„ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _combine_and_validate_industry_results(self, news_content: str, vector_candidates: List[Dict], 
                                             ai_candidates: List[Dict], industry_dict: Dict) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ AI í›„ë³´ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ê´€ë ¨ ì‚°ì—… ë„ì¶œ"""
        all_candidates = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for candidate in vector_candidates:
            name = candidate["name"]
            all_candidates[name] = {
                "name": name,
                "vector_similarity": candidate["similarity"],
                "ai_score": 0,
                "ai_reason": "",
                "description": candidate["description"]
            }
        
        # AI í›„ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for candidate in ai_candidates:
            name = candidate["industry"]
            if name in all_candidates:
                all_candidates[name]["ai_score"] = candidate["score"]
                all_candidates[name]["ai_reason"] = candidate["reason"]
            elif name in industry_dict:  # ìœ íš¨í•œ ì‚°ì—…ëª…ì¸ ê²½ìš°ë§Œ
                all_candidates[name] = {
                    "name": name,
                    "vector_similarity": 0,
                    "ai_score": candidate["score"],
                    "ai_reason": candidate["reason"],
                    "description": industry_dict[name]
                }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ë²¡í„° ìœ ì‚¬ë„ + AI ì ìˆ˜)
        for candidate in all_candidates.values():
            # ë²¡í„° ìœ ì‚¬ë„ë¥¼ 10ì  ë§Œì ìœ¼ë¡œ ì •ê·œí™”
            normalized_vector = candidate["vector_similarity"] / 10
            # AI ì ìˆ˜ëŠ” ì´ë¯¸ 10ì  ë§Œì 
            ai_score = candidate["ai_score"]
            
            # ê°€ì¤‘í‰ê·  (AI ì ìˆ˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            candidate["final_score"] = round((normalized_vector * 0.3 + ai_score * 0.7), 1)
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        sorted_candidates = sorted(all_candidates.values(), 
                                  key=lambda x: x["final_score"], 
                                  reverse=True)
        
        return sorted_candidates
    
    def _combine_and_validate_past_issue_results(self, news_content: str, vector_candidates: List[Dict], 
                                               ai_candidates: List[Dict], issue_dict: Dict) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ AI í›„ë³´ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ë„ì¶œ"""
        all_candidates = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for candidate in vector_candidates:
            name = candidate["name"]
            all_candidates[name] = {
                "name": name,
                "vector_similarity": candidate["similarity"],
                "ai_score": 0,
                "ai_reason": "",
                "description": candidate["description"],
                "period": candidate.get("period", "N/A")
            }
        
        # AI í›„ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for candidate in ai_candidates:
            name = candidate["issue"]
            if name in all_candidates:
                all_candidates[name]["ai_score"] = candidate["score"]
                all_candidates[name]["ai_reason"] = candidate["reason"]
            elif name in issue_dict:  # ìœ íš¨í•œ ì´ìŠˆëª…ì¸ ê²½ìš°ë§Œ
                all_candidates[name] = {
                    "name": name,
                    "vector_similarity": 0,
                    "ai_score": candidate["score"],
                    "ai_reason": candidate["reason"],
                    "description": issue_dict[name],
                    "period": "N/A"
                }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        for candidate in all_candidates.values():
            normalized_vector = candidate["vector_similarity"] / 10
            ai_score = candidate["ai_score"]
            candidate["final_score"] = round((normalized_vector * 0.3 + ai_score * 0.7), 1)
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        sorted_candidates = sorted(all_candidates.values(), 
                                  key=lambda x: x["final_score"], 
                                  reverse=True)
        
        return sorted_candidates

class IntegratedNewsPipeline:
    """í†µí•© ë‰´ìŠ¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ì‹¤ì œ RAG ë¶„ì„ í¬í•¨)"""
    
    def __init__(self, data_dir: str = "data2", headless: bool = True):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            headless: í¬ë¡¤ë§ ì‹œ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        project_root = Path(__file__).parent.parent  # scriptsì˜ ìƒìœ„ í´ë”
        self.data_dir = project_root / data_dir
        self.data_dir.mkdir(exist_ok=True)
        self.headless = headless
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.crawler = None
        self.filter = None
        self.rag_executor = RealRAGAnalysisExecutor(str(self.data_dir))  # ì‹¤ì œ RAG ì‹¤í–‰ê¸° ì‚¬ìš©
        
        # ì‹¤í–‰ ê²°ê³¼ ì €ì¥
        self.pipeline_results = {
            "pipeline_id": None,
            "started_at": None,
            "completed_at": None,
            "execution_time": None,
            "steps_completed": [],
            "final_status": None,
            "crawling_result": None,
            "filtering_result": None,
            "rag_analysis_result": None,
            "api_ready_data": None,
            "errors": []
        }

    def run_full_pipeline(self, 
                         issues_per_category: int = 10,
                         target_filtered_count: int = 5,
                         force_new_crawling: bool = False) -> Dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹¤ì œ RAG ë¶„ì„ í¬í•¨)
        
        Args:
            issues_per_category: ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§í•  ì´ìŠˆ ìˆ˜
            target_filtered_count: í•„í„°ë§í•  ìµœì¢… ì´ìŠˆ ìˆ˜
            force_new_crawling: ê°•ì œ ìƒˆ í¬ë¡¤ë§ ì—¬ë¶€
            
        Returns:
            íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼
        """
        # íŒŒì´í”„ë¼ì¸ ì‹œì‘
        pipeline_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.pipeline_results["pipeline_id"] = pipeline_id
        self.pipeline_results["started_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"ğŸš€ í†µí•© ë‰´ìŠ¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì‹¤ì œ RAG + GPT-4o-mini, ID: {pipeline_id})")
        print(f"{'='*80}")
        print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ ì„¤ì •:")
        print(f"   â€¢ ì¹´í…Œê³ ë¦¬ë³„ ì´ìŠˆ ìˆ˜: {issues_per_category}ê°œ")
        print(f"   â€¢ ìµœì¢… ì„ ë³„ ì´ìŠˆ ìˆ˜: {target_filtered_count}ê°œ")
        print(f"   â€¢ ì‹¤ì œ RAG ë¶„ì„ í¬í•¨: ì˜ˆ")
        print(f"   â€¢ AI ëª¨ë¸: GPT-4o-mini")
        print(f"   â€¢ ê°•ì œ ìƒˆ í¬ë¡¤ë§: {'ì˜ˆ' if force_new_crawling else 'ì•„ë‹ˆì˜¤'}")
        
        start_time = datetime.now()
        
        try:
            # Step 1: í¬ë¡¤ë§ (ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ë¡œë“œ)
            crawling_result = self._execute_crawling_step(
                issues_per_category, force_new_crawling
            )
            
            # Step 2: AI í•„í„°ë§
            filtering_result = self._execute_filtering_step(
                crawling_result, target_filtered_count
            )
            
            # Step 3: ì‹¤ì œ RAG ë¶„ì„
            rag_result = self._execute_real_rag_analysis_step(filtering_result)
            
            # Step 4: APIìš© ë°ì´í„° ì¤€ë¹„
            api_data = self._prepare_api_data(rag_result)
            
            # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
            end_time = datetime.now()
            execution_time = end_time - start_time
            
            self.pipeline_results.update({
                "completed_at": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "execution_time": str(execution_time),
                "final_status": "success",
                "crawling_result": crawling_result,
                "filtering_result": filtering_result,
                "rag_analysis_result": rag_result,
                "api_ready_data": api_data
            })
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self._save_pipeline_results()
            self.pipeline_results["saved_file"] = saved_file
            
            self._print_pipeline_summary()
            
            return self.pipeline_results
            
        except Exception as e:
            error_msg = f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            traceback.print_exc()
            
            self.pipeline_results.update({
                "completed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "final_status": "failed",
                "errors": [error_msg]
            })
            
            raise

    def _execute_crawling_step(self, issues_per_category: int, force_new: bool) -> Dict:
        """Step 1: í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ“¡ Step 1: ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§")
        print(f"{'='*60}")
        
        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸ (force_newê°€ Falseì¸ ê²½ìš°)
            if not force_new:
                existing_data = self._check_recent_crawling_data()
                if existing_data:
                    print(f"â™»ï¸ ìµœê·¼ í¬ë¡¤ë§ ë°ì´í„° ë°œê²¬, ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.pipeline_results["steps_completed"].append("crawling_reused")
                    return existing_data
            
            # ìƒˆ í¬ë¡¤ë§ ì‹¤í–‰
            print(f"ğŸ•·ï¸ ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‹œì‘...")
            self.crawler = BigKindsCrawler(
                data_dir=str(self.data_dir),
                headless=self.headless,
                issues_per_category=issues_per_category
            )
            
            crawling_result = self.crawler.crawl_all_categories()
            
            print(f"âœ… Step 1 ì™„ë£Œ: {crawling_result['total_issues']}ê°œ ì´ìŠˆ ìˆ˜ì§‘")
            self.pipeline_results["steps_completed"].append("crawling_new")
            
            return crawling_result
            
        except Exception as e:
            error_msg = f"í¬ë¡¤ë§ ë‹¨ê³„ ì‹¤íŒ¨: {e}"
            self.pipeline_results["errors"].append(error_msg)
            raise Exception(error_msg)

    def _execute_filtering_step(self, crawling_result: Dict, target_count: int) -> Dict:
        """Step 2: AI í•„í„°ë§ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ¤– Step 2: AI ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± í•„í„°ë§")
        print(f"{'='*60}")
        
        try:
            if not crawling_result or not crawling_result.get("all_issues"):
                raise ValueError("í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"ğŸ“Š í•„í„°ë§ ëŒ€ìƒ: {len(crawling_result['all_issues'])}ê°œ ì´ìŠˆ")
            print(f"ğŸ¯ ì„ ë³„ ëª©í‘œ: {target_count}ê°œ ì´ìŠˆ")
            
            self.filter = StockMarketFilter()
            filtering_result = self.filter.filter_issues_by_stock_relevance(
                crawling_result["all_issues"], target_count
            )
            
            # í•„í„°ë§ ê²°ê³¼ ì €ì¥
            saved_filter_file = self.filter.save_filtered_results(
                filtering_result, str(self.data_dir)
            )
            filtering_result["saved_file"] = saved_filter_file
            
            print(f"âœ… Step 2 ì™„ë£Œ: {len(filtering_result['selected_issues'])}ê°œ ì´ìŠˆ ì„ ë³„")
            self.pipeline_results["steps_completed"].append("filtering")
            
            return filtering_result
            
        except Exception as e:
            error_msg = f"í•„í„°ë§ ë‹¨ê³„ ì‹¤íŒ¨: {e}"
            self.pipeline_results["errors"].append(error_msg)
            raise Exception(error_msg)

    def _execute_real_rag_analysis_step(self, filtering_result: Dict) -> Dict:
        """Step 3: ì‹¤ì œ RAG ë¶„ì„ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ” Step 3: ì‹¤ì œ RAG ë¶„ì„ (ì‚°ì—… + ê³¼ê±° ì´ìŠˆ)")
        print(f"{'='*60}")
        
        try:
            selected_issues = filtering_result.get("selected_issues", [])
            if not selected_issues:
                raise ValueError("í•„í„°ë§ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"ğŸ“Š ì‹¤ì œ RAG ë¶„ì„ ëŒ€ìƒ: {len(selected_issues)}ê°œ ì„ ë³„ ì´ìŠˆ")
            
            # ê° ì´ìŠˆë³„ë¡œ ì‹¤ì œ RAG ë¶„ì„ ì‹¤í–‰
            enriched_issues = []
            
            for i, issue in enumerate(selected_issues, 1):
                print(f"ğŸ”„ ì´ìŠˆ {i}/{len(selected_issues)} ë¶„ì„ ì¤‘: {issue.get('ì œëª©', 'N/A')[:50]}...")
                
                # ì‹¤ì œ ì‚°ì—… ë¶„ì„
                related_industries = self.rag_executor.analyze_industry_for_issue(issue)
                
                # ì‹¤ì œ ê³¼ê±° ì´ìŠˆ ë¶„ì„
                related_past_issues = self.rag_executor.analyze_past_issues_for_issue(issue)
                
                # RAG ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°
                rag_confidence = self._calculate_rag_confidence(related_industries, related_past_issues)
                
                # ê¸°ë³¸ ì´ìŠˆ ì •ë³´ì— ì‹¤ì œ RAG ê²°ê³¼ ì¶”ê°€
                enriched_issue = issue.copy()
                enriched_issue["ê´€ë ¨ì‚°ì—…"] = related_industries
                enriched_issue["ê´€ë ¨ê³¼ê±°ì´ìŠˆ"] = related_past_issues
                enriched_issue["RAGë¶„ì„ì‹ ë¢°ë„"] = rag_confidence
                
                enriched_issues.append(enriched_issue)
                
                print(f"   âœ… ì´ìŠˆ {i} RAG ë¶„ì„ ì™„ë£Œ: ì‚°ì—… {len(related_industries)}ê°œ, ê³¼ê±°ì´ìŠˆ {len(related_past_issues)}ê°œ, ì‹ ë¢°ë„ {rag_confidence}")
            
            rag_result = {
                **filtering_result,  # ê¸°ì¡´ í•„í„°ë§ ê²°ê³¼ ìœ ì§€
                "selected_issues": enriched_issues,  # ì‹¤ì œ RAG ë¶„ì„ ê²°ê³¼ë¡œ êµì²´
                "rag_metadata": {
                    "real_rag_analysis": True,
                    "analysis_method": "vector_search + AI_agent + GPT-4o-mini",
                    "rag_completed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "enriched_issues_count": len(enriched_issues),
                    "average_confidence": round(sum(issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0) for issue in enriched_issues) / len(enriched_issues), 2) if enriched_issues else 0
                }
            }
            
            # RAG ë¶„ì„ ê²°ê³¼ ì €ì¥
            saved_rag_file = self._save_rag_results(rag_result)
            rag_result["rag_saved_file"] = saved_rag_file
            
            print(f"âœ… Step 3 ì™„ë£Œ: {len(enriched_issues)}ê°œ ì´ìŠˆ ì‹¤ì œ RAG ë¶„ì„ ì™„ë£Œ")
            print(f"ğŸ“Š í‰ê·  RAG ì‹ ë¢°ë„: {rag_result['rag_metadata']['average_confidence']}")
            self.pipeline_results["steps_completed"].append("real_rag_analysis")
            
            return rag_result
            
        except Exception as e:
            error_msg = f"ì‹¤ì œ RAG ë¶„ì„ ë‹¨ê³„ ì‹¤íŒ¨: {e}"
            self.pipeline_results["errors"].append(error_msg)
            raise Exception(error_msg)

    def _calculate_rag_confidence(self, industries: List[Dict], past_issues: List[Dict]) -> float:
        """RAG ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚° (ì‹¤ì œ ì ìˆ˜ ê¸°ë°˜)"""
        if not industries or not past_issues:
            return 0.0
        
        # ì‹¤ì œ final_score ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        industry_avg = sum(ind.get("final_score", 0) for ind in industries) / len(industries)
        past_avg = sum(issue.get("final_score", 0) for issue in past_issues) / len(past_issues)
        
        return round((industry_avg + past_avg) / 2, 1)

    def _save_rag_results(self, rag_result: Dict) -> str:
        """RAG ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_RealRAG_Enhanced_5issues.json"
            filepath = self.data_dir / filename
            
            save_data = {
                **rag_result,
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "rag_version": "RealRAG_Enhanced_Pipeline_v2.0",
                    "ai_model": "gpt-4o-mini"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ì‹¤ì œ RAG ë¶„ì„ ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âš ï¸ RAG ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""

    def _prepare_api_data(self, rag_result: Dict) -> Dict:
        """Step 4: APIìš© ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ RAG ê²°ê³¼ í¬í•¨)"""
        print(f"\n{'='*60}")
        print(f"ğŸŒ Step 4: API ì‘ë‹µ ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ RAG í¬í•¨)")
        print(f"{'='*60}")
        
        try:
            selected_issues = rag_result.get("selected_issues", [])
            
            # API ì‘ë‹µ í˜•íƒœë¡œ ë°ì´í„° ë³€í™˜
            api_data = {
                "success": True,
                "data": {
                    "total_crawled": rag_result.get("original_issues_count", 0),
                    "selected_count": len(selected_issues),
                    "selection_criteria": "ì£¼ì‹ì‹œì¥ ì˜í–¥ë„ + ì‹¤ì œ RAG ë¶„ì„",
                    "selected_issues": []
                },
                "metadata": {
                    "crawled_at": rag_result.get("filter_metadata", {}).get("filtered_at", ""),
                    "categories_processed": getattr(BigKindsCrawler, 'TARGET_CATEGORIES', []),
                    "ai_filter_applied": True,
                    "real_rag_analysis_applied": True,
                    "filter_model": "gpt-4o-mini",
                    "rag_model": "gpt-4o-mini",
                    "rag_confidence": self._calculate_overall_rag_confidence(selected_issues)
                }
            }
            
            # ì´ìŠˆ ë°ì´í„° ë³€í™˜ (ì‹¤ì œ RAG ê²°ê³¼ í¬í•¨)
            for issue in selected_issues:
                api_issue = {
                    "ì´ìŠˆë²ˆí˜¸": issue.get("ì´ìŠˆë²ˆí˜¸", 0),
                    "ì œëª©": issue.get("ì œëª©", ""),
                    "ë‚´ìš©": issue.get("ì›ë³¸ë‚´ìš©", issue.get("ë‚´ìš©", "")),
                    "ì¹´í…Œê³ ë¦¬": issue.get("ì¹´í…Œê³ ë¦¬", ""),
                    "ì¶”ì¶œì‹œê°„": issue.get("ì¶”ì¶œì‹œê°„", ""),
                    "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": issue.get("ì¢…í•©ì ìˆ˜", 0),
                    "ìˆœìœ„": issue.get("rank", 0),
                    
                    # ì‹¤ì œ RAG ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    "ê´€ë ¨ì‚°ì—…": issue.get("ê´€ë ¨ì‚°ì—…", []),
                    "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": issue.get("ê´€ë ¨ê³¼ê±°ì´ìŠˆ", []),
                    "RAGë¶„ì„ì‹ ë¢°ë„": issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0.0),
                }
                api_data["data"]["selected_issues"].append(api_issue)
            
            # API ë°ì´í„° ì •ë ¬ (ìˆœìœ„ë³„)
            api_data["data"]["selected_issues"].sort(key=lambda x: x.get("ìˆœìœ„", 999))
            
            print(f"âœ… Step 4 ì™„ë£Œ: ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ API ì‘ë‹µ ë°ì´í„° ì¤€ë¹„")
            self.pipeline_results["steps_completed"].append("api_preparation")
            
            return api_data
            
        except Exception as e:
            error_msg = f"API ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}"
            self.pipeline_results["errors"].append(error_msg)
            raise Exception(error_msg)

    def _calculate_overall_rag_confidence(self, selected_issues: List[Dict]) -> float:
        """ì „ì²´ RAG ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not selected_issues:
            return 0.0
        
        confidences = [issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0.0) for issue in selected_issues]
        return round(sum(confidences) / len(confidences), 2)

    def _check_recent_crawling_data(self, max_age_hours: int = 6) -> Optional[Dict]:
        """ìµœê·¼ í¬ë¡¤ë§ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            json_files = list(self.data_dir.glob("*_MultiCategory_*issues.json"))
            if not json_files:
                return None
            
            # ê°€ì¥ ìµœì‹  íŒŒì¼ í™•ì¸
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            file_age = datetime.now() - datetime.fromtimestamp(latest_file.stat().st_mtime)
            
            # ì§€ì •ëœ ì‹œê°„ë³´ë‹¤ ì˜¤ë˜ëœ ê²½ìš° ìƒˆ í¬ë¡¤ë§ í•„ìš”
            if file_age > timedelta(hours=max_age_hours):
                print(f"ğŸ“… ê¸°ì¡´ ë°ì´í„°ê°€ {file_age}ë§Œí¼ ì˜¤ë˜ë˜ì–´ ìƒˆ í¬ë¡¤ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"â™»ï¸ {file_age}ë§Œí¼ ì˜¤ë˜ëœ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return data
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

    def _save_pipeline_results(self) -> str:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_RealRAG_Pipeline_Results.json"
            filepath = self.data_dir / filename
            
            save_data = {
                **self.pipeline_results,
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "pipeline_version": "RealRAG_IntegratedNewsPipeline_v2.0",
                    "ai_model": "gpt-4o-mini"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""

    def _print_pipeline_summary(self):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ì‹¤ì œ RAG í†µí•© ë‰´ìŠ¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"{'='*80}")
        
        results = self.pipeline_results
        print(f"ğŸ†” íŒŒì´í”„ë¼ì¸ ID: {results['pipeline_id']}")
        print(f"â° ì‹¤í–‰ ì‹œê°„: {results['execution_time']}")
        print(f"ğŸ“Š ìµœì¢… ìƒíƒœ: {results['final_status']}")
        print(f"ğŸ¤– AI ëª¨ë¸: GPT-4o-mini")
        
        print(f"\nâœ… ì™„ë£Œëœ ë‹¨ê³„:")
        for step in results["steps_completed"]:
            step_names = {
                "crawling_new": "ğŸ•·ï¸ ìƒˆ í¬ë¡¤ë§ ì‹¤í–‰",
                "crawling_reused": "â™»ï¸ ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ì¬ì‚¬ìš©",
                "filtering": "ğŸ¤– AI í•„í„°ë§",
                "real_rag_analysis": "ğŸ” ì‹¤ì œ RAG ë¶„ì„ (ì‚°ì—… + ê³¼ê±° ì´ìŠˆ)",
                "api_preparation": "ğŸŒ API ë°ì´í„° ì¤€ë¹„"
            }
            print(f"   â€¢ {step_names.get(step, step)}")
        
        # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
        if results.get("api_ready_data"):
            api_data = results["api_ready_data"]
            print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
            print(f"   â€¢ í¬ë¡¤ë§ëœ ì´ ì´ìŠˆ: {api_data['data']['total_crawled']}ê°œ")
            print(f"   â€¢ AI ì„ ë³„ ì´ìŠˆ: {api_data['data']['selected_count']}ê°œ")
            print(f"   â€¢ ì‹¤ì œ RAG ë¶„ì„ ì‹ ë¢°ë„: {api_data['metadata']['rag_confidence']}")
            
            # TOP 3 ì´ìŠˆ ë¯¸ë¦¬ë³´ê¸° (ì‹¤ì œ RAG ê²°ê³¼ í¬í•¨)
            selected_issues = api_data["data"]["selected_issues"]
            if selected_issues:
                print(f"\nğŸ† TOP 3 ì„ ë³„ ì´ìŠˆ (ì‹¤ì œ RAG ë¶„ì„ í¬í•¨):")
                for issue in selected_issues[:3]:
                    print(f"   {issue['ìˆœìœ„']}. [{issue['ì¹´í…Œê³ ë¦¬']}] {issue['ì œëª©'][:50]}...")
                    print(f"      ğŸ’° ê´€ë ¨ì„± ì ìˆ˜: {issue['ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜']}")
                    
                    industries = issue.get('ê´€ë ¨ì‚°ì—…', [])
                    past_issues = issue.get('ê´€ë ¨ê³¼ê±°ì´ìŠˆ', [])
                    
                    if industries:
                        industry_names = [ind.get('name', 'N/A') for ind in industries]
                        print(f"      ğŸ­ ê´€ë ¨ ì‚°ì—…: {', '.join(industry_names)}")
                    else:
                        print(f"      ğŸ­ ê´€ë ¨ ì‚°ì—…: ì—†ìŒ")
                        
                    if past_issues:
                        past_names = [past.get('name', 'N/A') for past in past_issues]
                        print(f"      ğŸ“š ê´€ë ¨ ê³¼ê±°ì´ìŠˆ: {', '.join(past_names)}")
                    else:
                        print(f"      ğŸ“š ê´€ë ¨ ê³¼ê±°ì´ìŠˆ: ì—†ìŒ")
                        
                    print(f"      ğŸ” RAG ì‹ ë¢°ë„: {issue.get('RAGë¶„ì„ì‹ ë¢°ë„', 0.0)}")
        
        # ì—ëŸ¬ê°€ ìˆë‹¤ë©´ í‘œì‹œ
        if results.get("errors"):
            print(f"\nâš ï¸ ë°œìƒí•œ ì˜¤ë¥˜:")
            for error in results["errors"]:
                print(f"   â€¢ {error}")

    def get_latest_api_data(self) -> Optional[Dict]:
        """ìµœì‹  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ì—ì„œ API ë°ì´í„° ì¶”ì¶œ"""
        try:
            json_files = list(self.data_dir.glob("*_RealRAG_Pipeline_Results.json"))
            if not json_files:
                # RAG ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ í™•ì¸
                json_files = list(self.data_dir.glob("*_Pipeline_Results.json"))
            
            if not json_files:
                return None
            
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return data.get("api_ready_data")
            
        except Exception as e:
            print(f"âŒ API ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def run_quick_update(self, force_crawling: bool = False) -> Dict:
        """ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì‹¤ì œ RAG ë¶„ì„ í¬í•¨)"""
        print("ğŸš€ ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì‹¤ì œ RAG + GPT-4o-mini)...")
        
        try:
            return self.run_full_pipeline(
                issues_per_category=10,
                target_filtered_count=5,
                force_new_crawling=force_crawling
            )
        except Exception as e:
            print(f"âŒ ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°ì´í„°ë¼ë„ ë°˜í™˜ ì‹œë„
            existing_data = self.get_latest_api_data()
            if existing_data:
                print("â™»ï¸ ê¸°ì¡´ API ë°ì´í„° ë°˜í™˜")
                return {"api_ready_data": existing_data, "final_status": "fallback"}
            raise

# í¸ì˜ í•¨ìˆ˜ë“¤
def run_full_news_pipeline_with_rag(headless: bool = True, 
                                   force_new_crawling: bool = False,
                                   issues_per_category: int = 10) -> Dict:
    """ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ì „ì²´ ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    pipeline = IntegratedNewsPipeline(headless=headless)
    return pipeline.run_full_pipeline(
        issues_per_category=issues_per_category,
        force_new_crawling=force_new_crawling
    )

def get_latest_rag_enhanced_issues_for_api():
    """ì‹¤ì œ RAG ë¶„ì„ì´ í¬í•¨ëœ ìµœì‹  ì´ìŠˆ ë°ì´í„° ë°˜í™˜"""
    try:
        # 1. ì‹¤ì œ RAG Enhanced íŒŒì¼ ì§ì ‘ ì°¾ê¸°
        pipeline = IntegratedNewsPipeline(headless=True)
        data_dir = pipeline.data_dir
        
        # ì‹¤ì œ RAG Enhanced JSON íŒŒì¼ë“¤ ì°¾ê¸°
        real_rag_files = list(data_dir.glob("*_RealRAG_Enhanced_*issues.json"))
        
        if real_rag_files:
            # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_file = max(real_rag_files, key=lambda f: f.stat().st_mtime)
            print(f"âœ… ì‹¤ì œ RAG ë¶„ì„ ë°ì´í„° ë°œê²¬: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # selected_issues ë°˜í™˜ (ì‹¤ì œ RAG ê²°ê³¼ í¬í•¨)
            selected_issues = data.get('selected_issues', [])
            print(f"ğŸ“Š ì‹¤ì œ RAG ë¶„ì„ëœ {len(selected_issues)}ê°œ ì´ìŠˆ ë°˜í™˜")
            return selected_issues
            
        # 2. ì‹¤ì œ RAG ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ StockFiltered ë°ì´í„° í™•ì¸
        print("ğŸ” ê¸°ì¡´ í•„í„°ë§ ë°ì´í„° í™•ì¸ ì¤‘...")
        filtered_files = list(data_dir.glob("*_StockFiltered_*issues.json"))
        
        if filtered_files:
            latest_file = max(filtered_files, key=lambda f: f.stat().st_mtime)
            print(f"ğŸ“Š ê¸°ì¡´ í•„í„°ë§ ë°ì´í„° ë°œê²¬: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            selected_issues = data.get('selected_issues', [])
            print(f"âš ï¸ RAG ë¶„ì„ ì—†ëŠ” {len(selected_issues)}ê°œ ì´ìŠˆ ë°˜í™˜ (ê¸°ë³¸ í•„í„°ë§ë§Œ)")
            return selected_issues
            
        # 3. í•„í„°ë§ ë°ì´í„°ë„ ì—†ìœ¼ë©´ ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° í™•ì¸
        print("ğŸ” ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° í™•ì¸ ì¤‘...")
        try:
            from crawling_bigkinds import BigKindsCrawler
            crawler = BigKindsCrawler()
            raw_data = crawler.load_latest_results()
            
            if raw_data and raw_data.get('all_issues'):
                print(f"ğŸ“Š ì›ë³¸ ë°ì´í„° {len(raw_data['all_issues'])}ê°œ ë°œê²¬, ìƒìœ„ 5ê°œ ë°˜í™˜")
                # ì›ë³¸ ë°ì´í„°ì—ì„œ ìƒìœ„ 5ê°œë§Œ ë°˜í™˜ (ì„ì‹œ)
                issues = raw_data['all_issues'][:5]
                return [{
                    "ì´ìŠˆë²ˆí˜¸": issue.get("ì´ìŠˆë²ˆí˜¸", i+1),
                    "ì œëª©": issue.get("ì œëª©", ""),
                    "ë‚´ìš©": issue.get("ë‚´ìš©", ""),
                    "ì¹´í…Œê³ ë¦¬": issue.get("ì¹´í…Œê³ ë¦¬", ""),
                    "ì¶”ì¶œì‹œê°„": issue.get("ì¶”ì¶œì‹œê°„", ""),
                    "ìˆœìœ„": i+1,
                    "ê´€ë ¨ì‚°ì—…": [],  # ë¹ˆ ë°°ì—´
                    "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": [],  # ë¹ˆ ë°°ì—´
                    "RAGë¶„ì„ì‹ ë¢°ë„": 0.0
                } for i, issue in enumerate(issues)]
        except Exception as e:
            print(f"âš ï¸ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        # 4. ì•„ë¬´ ë°ì´í„°ë„ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„°
        print("âš ï¸ ë°ì´í„° ì—†ìŒ, ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
        return [
            {
                "ì´ìŠˆë²ˆí˜¸": 1,
                "ì œëª©": "SKí•˜ì´ë‹‰ìŠ¤ AI ë°˜ë„ì²´ ìˆ˜ìš” ê¸‰ì¦",
                "ë‚´ìš©": "SKí•˜ì´ë‹‰ìŠ¤ê°€ AI ë°˜ë„ì²´ ìˆ˜ìš” ì¦ê°€ë¡œ ë¶„ê¸° ìµœëŒ€ ì‹¤ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤...",
                "ì¹´í…Œê³ ë¦¬": "ê²½ì œ",
                "ìˆœìœ„": 1,
                "ì¶”ì¶œì‹œê°„": datetime.now().isoformat(),
                "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": 9.2,
                "ê´€ë ¨ì‚°ì—…": [
                    {
                        "name": "ë°˜ë„ì²´",
                        "final_score": 9.1,
                        "ai_reason": "AI ë°˜ë„ì²´ ìˆ˜ìš”ì™€ ì§ì ‘ì  ì—°ê´€",
                        "vector_similarity": 91.2
                    }
                ],
                "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": [
                    {
                        "name": "2022ë…„ ë°˜ë„ì²´ ê³µê¸‰ë‚œ",
                        "final_score": 8.8,
                        "ai_reason": "ë°˜ë„ì²´ ìˆ˜ê¸‰ ë¶ˆê· í˜• íŒ¨í„´ ìœ ì‚¬",
                        "vector_similarity": 88.5,
                        "period": "2022.03 ~ 2022.12"
                    }
                ],
                "RAGë¶„ì„ì‹ ë¢°ë„": 8.9
            },
            {
                "ì´ìŠˆë²ˆí˜¸": 2,
                "ì œëª©": "í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê¸°ìˆ  í˜ì‹ ",
                "ë‚´ìš©": "í˜„ëŒ€ì°¨ê°€ ì°¨ì„¸ëŒ€ ë°°í„°ë¦¬ ê¸°ìˆ  ê°œë°œë¡œ ì „ê¸°ì°¨ ì‹œì¥ ì„ ë„ë¥¼ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "ì¹´í…Œê³ ë¦¬": "ê²½ì œ",
                "ìˆœìœ„": 2,
                "ì¶”ì¶œì‹œê°„": datetime.now().isoformat(),
                "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": 8.7,
                "ê´€ë ¨ì‚°ì—…": [
                    {
                        "name": "ìë™ì°¨",
                        "final_score": 8.9,
                        "ai_reason": "ì „ê¸°ì°¨ ê¸°ìˆ  í˜ì‹ ê³¼ ì§ì ‘ì  ì—°ê´€",
                        "vector_similarity": 89.3
                    }
                ],
                "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": [
                    {
                        "name": "2021ë…„ ì „ê¸°ì°¨ ë³´ê¸‰ í™•ì‚°",
                        "final_score": 8.2,
                        "ai_reason": "ì „ê¸°ì°¨ ì‹œì¥ ì„±ì¥ íŒ¨í„´ ìœ ì‚¬",
                        "vector_similarity": 82.4,
                        "period": "2021.01 ~ 2021.12"
                    }
                ],
                "RAGë¶„ì„ì‹ ë¢°ë„": 8.5
            }
        ]
        
    except Exception as e:
        print(f"ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        return []

def quick_refresh_rag_news_data(force_crawling: bool = False) -> Dict:
    """ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ë‰´ìŠ¤ ë°ì´í„° ë¹ ë¥¸ ìƒˆë¡œê³ ì¹¨"""
    pipeline = IntegratedNewsPipeline(headless=True)
    return pipeline.run_quick_update(force_crawling=force_crawling)

# ìŠ¤ì¼€ì¤„ë§ìš© í•¨ìˆ˜
def scheduled_daily_update_with_rag():
    """ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ì¼ì¼ ì •ê¸° ì—…ë°ì´íŠ¸ìš© í•¨ìˆ˜"""
    print("ğŸ“… ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ì¼ì¼ ì •ê¸° ì—…ë°ì´íŠ¸ ì‹œì‘...")
    try:
        result = run_full_news_pipeline_with_rag(
            headless=True,
            force_new_crawling=True,  # ì •ê¸° ì—…ë°ì´íŠ¸ëŠ” í•­ìƒ ìƒˆ í¬ë¡¤ë§
            issues_per_category=10
        )
        print("âœ… ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ì¼ì¼ ì •ê¸° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        return result
    except Exception as e:
        print(f"âŒ ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ ì¼ì¼ ì •ê¸° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ”„ Real RAG Enhanced Integrated News Analysis Pipeline")
    print("="*80)
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        
        if mode == "quick":
            print("âš¡ ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ëª¨ë“œ (ì‹¤ì œ RAG + GPT-4o-mini)")
            result = quick_refresh_rag_news_data()
            
        elif mode == "force":
            print("ğŸ”„ ê°•ì œ ìƒˆ í¬ë¡¤ë§ ëª¨ë“œ (ì‹¤ì œ RAG + GPT-4o-mini)")
            result = run_full_news_pipeline_with_rag(force_new_crawling=True)
            
        elif mode == "daily":
            print("ğŸ“… ì¼ì¼ ì •ê¸° ì—…ë°ì´íŠ¸ ëª¨ë“œ (ì‹¤ì œ RAG + GPT-4o-mini)")
            result = scheduled_daily_update_with_rag()
            
        elif mode == "api":
            print("ğŸŒ ì‹¤ì œ RAG API ë°ì´í„° ì¡°íšŒ ëª¨ë“œ")
            api_data = get_latest_rag_enhanced_issues_for_api()
            if api_data:
                print("âœ… ì‹¤ì œ RAG API ë°ì´í„° ì¡°íšŒ ì„±ê³µ")
                print(f"ğŸ“Š ì‹¤ì œ RAG ë¶„ì„ëœ ì´ìŠˆ ìˆ˜: {len(api_data)}ê°œ")
                for issue in api_data[:3]:  # TOP 3ë§Œ ë¯¸ë¦¬ë³´ê¸°
                    print(f"   â€¢ {issue.get('ì œëª©', 'N/A')[:50]}...")
                    print(f"     ê´€ë ¨ì‚°ì—…: {len(issue.get('ê´€ë ¨ì‚°ì—…', []))}ê°œ")
                    print(f"     ê´€ë ¨ê³¼ê±°ì´ìŠˆ: {len(issue.get('ê´€ë ¨ê³¼ê±°ì´ìŠˆ', []))}ê°œ")
                    print(f"     RAG ì‹ ë¢°ë„: {issue.get('RAGë¶„ì„ì‹ ë¢°ë„', 0.0)}")
            else:
                print("âŒ ì‹¤ì œ RAG API ë°ì´í„° ì—†ìŒ")
            sys.exit(0)
            
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
            print("ì‚¬ìš©ë²•: python integrated_pipeline.py [quick|force|daily|api]")
            sys.exit(1)
    else:
        # ê¸°ë³¸ ëª¨ë“œ: ì‹¤ì œ RAG ë¶„ì„ í¬í•¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        print("ğŸ”„ ê¸°ë³¸ ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ ëª¨ë“œ")
        
        pipeline = IntegratedNewsPipeline(headless=False)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¸Œë¼ìš°ì € í‘œì‹œ
        
        try:
            result = pipeline.run_full_pipeline(
                issues_per_category=10,
                target_filtered_count=5,
                force_new_crawling=False
            )
            
            print(f"\nğŸ¯ ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            print(f"   â€¢ ìƒíƒœ: {result['final_status']}")
            print(f"   â€¢ ì‹¤í–‰ ì‹œê°„: {result['execution_time']}")
            print(f"   â€¢ AI ëª¨ë¸: GPT-4o-mini")
            
            if result.get("api_ready_data"):
                api_data = result["api_ready_data"]
                print(f"   â€¢ ìµœì¢… ì„ ë³„ ì´ìŠˆ: {api_data['data']['selected_count']}ê°œ")
                print(f"   â€¢ ì‹¤ì œ RAG ë¶„ì„ ì‹ ë¢°ë„: {api_data['metadata']['rag_confidence']}")
                
        except Exception as e:
            print(f"âŒ ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            sys.exit(1)