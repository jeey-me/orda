#!/usr/bin/env python3
"""
BigKinds 크롤러 모듈 - FastAPI 연동용
기존 스크립트를 클래스 형태로 모듈화 (카테고리 선택 기능 반영)
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time
import traceback
import json
from datetime import datetime
import asyncio
import os
import glob
from pathlib import Path
from typing import List, Dict, Optional

class BigKindsCrawler:
    def __init__(self, data_dir: str = "data", headless: bool = False):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.headless = headless
        self.driver = None
        self.wait = None

    def _setup_driver(self):
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
        else:
            options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.wait = WebDriverWait(self.driver, 10)

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except Exception as e:
                print(f"⚠️ 드라이버 종료 중 오류: {e}")
            finally:
                self.driver = None
                self.wait = None

    def crawl_current_issues(self, category: str = "경제", max_issues: int = 10) -> Dict:
        try:
            print(f"🚀 BigKinds 크롤링 시작 (카테고리: {category}, 최대 {max_issues}개 이슈)")
            self._setup_driver()
            self.driver.get("https://www.bigkinds.or.kr/")
            time.sleep(2)
            self._scroll_to_issues_section()
            self._click_category(category)
            results = self._crawl_issues(max_issues)
            saved_file = self.save_to_json(results, category=category)

            return {
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_issues": len(results),
                "source": "bigkinds.or.kr",
                "category": category,
                "issues": results,
                "saved_file": saved_file
            }
        except Exception as e:
            print(f"❌ 크롤링 실패: {e}")
            traceback.print_exc()
            raise
        finally:
            self._cleanup_driver()

    def _scroll_to_issues_section(self):
        self.driver.execute_script("window.scrollTo(0, 880);")
        time.sleep(1)
        print("✅ 스크롤 이동 완료")

    def _click_category(self, category: str):
        try:
            selector = f'a.issue-category[data-category="{category}"]'
            category_button = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
            self.driver.execute_script("arguments[0].click();", category_button)
            print(f"✅ 카테고리 클릭 완료: {category}")
            time.sleep(3)
        except Exception as e:
            print(f"❌ 카테고리 클릭 실패: {category}")
            raise

    def _crawl_issues(self, max_issues: int) -> List[Dict]:
        results = []
        for i in range(1, max_issues + 1):
            print(f"▶️ {i}번 이슈 처리 시작")
            try:
                if i >= 3:
                    self._navigate_slides(i)
                issue_data = self._extract_issue_data(i)
                if issue_data:
                    results.append(issue_data)
                self._close_popup_and_restore()
            except Exception as e:
                print(f"❌ {i}번 이슈 처리 중 오류 발생: {e}")
                continue
        return results

    def _navigate_slides(self, issue_num: int):
        for _ in range(issue_num - 3):
            try:
                next_btn = self.driver.find_element(By.CSS_SELECTOR, 'div.swiper-button-next.section2-btn.st2-sw1-next')
                is_disabled = next_btn.get_attribute('aria-disabled') == 'true'
                if is_disabled:
                    break
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(0.8)
            except Exception as e:
                print(f"⚠️ 슬라이드 넘기기 중 오류: {e}")
                break

    def _extract_issue_data(self, issue_num: int) -> Optional[Dict]:
        try:
            issue_selector = f'div.swiper-slide:nth-child({issue_num}) .issue-item-link'
            issue_element = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, issue_selector)))
            self.driver.execute_script("arguments[0].scrollIntoView(true);", issue_element)
            self.driver.execute_script("arguments[0].click();", issue_element)

            title_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.issuPopTitle')))
            content_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.pT20.issuPopContent')))

            return {
                "이슈번호": issue_num,
                "제목": title_elem.text.strip(),
                "내용": content_elem.text.strip(),
                "추출시간": datetime.now().isoformat()
            }
        except Exception as e:
            print(f"❌ 이슈 {issue_num} 데이터 추출 실패: {e}")
            return None

    def _close_popup_and_restore(self):
        try:
            ActionChains(self.driver).send_keys(Keys.ESCAPE).perform()
            time.sleep(1)
            self.driver.execute_script("window.scrollTo(0, 880);")
            time.sleep(1)
        except Exception as e:
            print(f"⚠️ 팝업 닫기 실패: {e}")

    def save_to_json(self, data: List[Dict], category: str = "경제") -> str:
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_BigKinds_{category}_issues.json"
            filepath = self.data_dir / filename
            save_data = {
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_issues": len(data),
                "source": "bigkinds.or.kr",
                "category": category,
                "issues": data
            }
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            print(f"✅ JSON 저장 완료: {filepath}")
            return str(filepath)
        except Exception as e:
            print(f"❌ JSON 저장 실패: {e}")
            raise

    def load_latest_issues(self) -> Optional[Dict]:
        try:
            json_files = list(self.data_dir.glob("*_BigKinds_*_issues.json"))
            if not json_files:
                print("📂 저장된 크롤링 데이터가 없습니다.")
                return None
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"✅ 최신 데이터 로드: {latest_file.name}")
            return data
        except Exception as e:
            print(f"❌ 데이터 로드 실패: {e}")
            return None

    def crawl_and_update(self):
        try:
            result = self.crawl_current_issues()
            print("🔄 크롤링 및 벡터 업데이트 완료")
            return result
        except Exception as e:
            print(f"❌ 크롤링 업데이트 실패: {e}")
            raise

# 테스트 실행용
if __name__ == "__main__":
    print("🧪 BigKinds 크롤러 테스트 모드")
    crawler = BigKindsCrawler(headless=False)
    result = crawler.crawl_current_issues(category="국제", max_issues=5)
    print("\n📊 크롤링 결과:")
    for issue in result["issues"]:
        print(f"• {issue['제목']}")
