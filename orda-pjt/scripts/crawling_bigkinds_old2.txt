#!/usr/bin/env python3
"""
BigKinds í¬ë¡¤ëŸ¬ ëª¨ë“ˆ - FastAPI ì—°ë™ìš©
ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í´ë˜ìŠ¤ í˜•íƒœë¡œ ëª¨ë“ˆí™” (ì¹´í…Œê³ ë¦¬ ì„ íƒ ê¸°ëŠ¥ ë°˜ì˜)
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time
import traceback
import json
from datetime import datetime
import asyncio
import os
import glob
from pathlib import Path
from typing import List, Dict, Optional

class BigKindsCrawler:
    def __init__(self, data_dir: str = "data", headless: bool = False):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.headless = headless
        self.driver = None
        self.wait = None

    def _setup_driver(self):
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument("--headless")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
        else:
            options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        self.wait = WebDriverWait(self.driver, 10)

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except Exception as e:
                print(f"âš ï¸ ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.driver = None
                self.wait = None

    def crawl_current_issues(self, category: str = "ê²½ì œ", max_issues: int = 10) -> Dict:
        try:
            print(f"ğŸš€ BigKinds í¬ë¡¤ë§ ì‹œì‘ (ì¹´í…Œê³ ë¦¬: {category}, ìµœëŒ€ {max_issues}ê°œ ì´ìŠˆ)")
            self._setup_driver()
            self.driver.get("https://www.bigkinds.or.kr/")
            time.sleep(2)
            self._scroll_to_issues_section()
            self._click_category(category)
            results = self._crawl_issues(max_issues)
            saved_file = self.save_to_json(results, category=category)

            return {
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_issues": len(results),
                "source": "bigkinds.or.kr",
                "category": category,
                "issues": results,
                "saved_file": saved_file
            }
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            raise
        finally:
            self._cleanup_driver()

    def _scroll_to_issues_section(self):
        self.driver.execute_script("window.scrollTo(0, 880);")
        time.sleep(1)
        print("âœ… ìŠ¤í¬ë¡¤ ì´ë™ ì™„ë£Œ")

    def _click_category(self, category: str):
        try:
            selector = f'a.issue-category[data-category="{category}"]'
            category_button = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
            self.driver.execute_script("arguments[0].click();", category_button)
            print(f"âœ… ì¹´í…Œê³ ë¦¬ í´ë¦­ ì™„ë£Œ: {category}")
            time.sleep(3)
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹¤íŒ¨: {category}")
            raise

    def _crawl_issues(self, max_issues: int) -> List[Dict]:
        results = []
        for i in range(1, max_issues + 1):
            print(f"â–¶ï¸ {i}ë²ˆ ì´ìŠˆ ì²˜ë¦¬ ì‹œì‘")
            try:
                if i >= 3:
                    self._navigate_slides(i)
                issue_data = self._extract_issue_data(i)
                if issue_data:
                    results.append(issue_data)
                self._close_popup_and_restore()
            except Exception as e:
                print(f"âŒ {i}ë²ˆ ì´ìŠˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        return results

    def _navigate_slides(self, issue_num: int):
        for _ in range(issue_num - 3):
            try:
                next_btn = self.driver.find_element(By.CSS_SELECTOR, 'div.swiper-button-next.section2-btn.st2-sw1-next')
                is_disabled = next_btn.get_attribute('aria-disabled') == 'true'
                if is_disabled:
                    break
                self.driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(0.8)
            except Exception as e:
                print(f"âš ï¸ ìŠ¬ë¼ì´ë“œ ë„˜ê¸°ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
                break

    def _extract_issue_data(self, issue_num: int) -> Optional[Dict]:
        try:
            issue_selector = f'div.swiper-slide:nth-child({issue_num}) .issue-item-link'
            issue_element = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, issue_selector)))
            self.driver.execute_script("arguments[0].scrollIntoView(true);", issue_element)
            self.driver.execute_script("arguments[0].click();", issue_element)

            title_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.issuPopTitle')))
            content_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.pT20.issuPopContent')))

            return {
                "ì´ìŠˆë²ˆí˜¸": issue_num,
                "ì œëª©": title_elem.text.strip(),
                "ë‚´ìš©": content_elem.text.strip(),
                "ì¶”ì¶œì‹œê°„": datetime.now().isoformat()
            }
        except Exception as e:
            print(f"âŒ ì´ìŠˆ {issue_num} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _close_popup_and_restore(self):
        try:
            ActionChains(self.driver).send_keys(Keys.ESCAPE).perform()
            time.sleep(1)
            self.driver.execute_script("window.scrollTo(0, 880);")
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ íŒì—… ë‹«ê¸° ì‹¤íŒ¨: {e}")

    def save_to_json(self, data: List[Dict], category: str = "ê²½ì œ") -> str:
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_BigKinds_{category}_issues.json"
            filepath = self.data_dir / filename
            save_data = {
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_issues": len(data),
                "source": "bigkinds.or.kr",
                "category": category,
                "issues": data
            }
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filepath}")
            return str(filepath)
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_latest_issues(self) -> Optional[Dict]:
        try:
            json_files = list(self.data_dir.glob("*_BigKinds_*_issues.json"))
            if not json_files:
                print("ğŸ“‚ ì €ì¥ëœ í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… ìµœì‹  ë°ì´í„° ë¡œë“œ: {latest_file.name}")
            return data
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def crawl_and_update(self):
        try:
            result = self.crawl_current_issues()
            print("ğŸ”„ í¬ë¡¤ë§ ë° ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return result
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©
if __name__ == "__main__":
    print("ğŸ§ª BigKinds í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    crawler = BigKindsCrawler(headless=False)
    result = crawler.crawl_current_issues(category="êµ­ì œ", max_issues=5)
    print("\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
    for issue in result["issues"]:
        print(f"â€¢ {issue['ì œëª©']}")
