#!/usr/bin/env python3
"""
RAG ë¶„ì„ ì‹œìŠ¤í…œ ëª¨ë“ˆ - FastAPI ì—°ë™ìš©
í˜„ì¬ ë‰´ìŠ¤ â†’ ê³¼ê±° ì´ìŠˆ ë§¤ì¹­ â†’ ì‚°ì—… ì—°ê²° â†’ ì¢…í•© ë¶„ì„
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback
import openai

from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class SimpleCSVAnalyzer:
    def __init__(self, industry_csv_path, past_issue_csv_path):
        self.industry_df = pd.read_csv(industry_csv_path)
        self.past_df = pd.read_csv(past_issue_csv_path)
        
        # ì‚°ì—… ì„¤ëª… ë²¡í„°í™”
        self.industry_vectorizer = TfidfVectorizer()
        self.industry_tfidf = self.industry_vectorizer.fit_transform(self.industry_df['ìƒì„¸ë‚´ìš©'].fillna(''))

        # ê³¼ê±° ë‰´ìŠ¤ ë²¡í„°í™”
        self.past_vectorizer = TfidfVectorizer()
        self.past_tfidf = self.past_vectorizer.fit_transform(self.past_df['Contents'].fillna(''))

    def analyze(self, news_content, top_k=3):
        # ì‚°ì—… ë¶„ì„
        input_vec = self.industry_vectorizer.transform([news_content])
        industry_sim = cosine_similarity(input_vec, self.industry_tfidf)[0]
        industry_results = self.industry_df.copy()
        industry_results['score'] = industry_sim
        top_industries = industry_results.sort_values('score', ascending=False).head(top_k)

        # ê³¼ê±° ì´ìŠˆ ë¶„ì„
        input_vec2 = self.past_vectorizer.transform([news_content])
        past_sim = cosine_similarity(input_vec2, self.past_tfidf)[0]
        past_results = self.past_df.copy()
        past_results['score'] = past_sim
        top_past = past_results.sort_values('score', ascending=False).head(top_k)

        return {
            "related_industries": top_industries[['KRX ì—…ì¢…ëª…', 'ìƒì„¸ë‚´ìš©', 'score']].rename(
    columns={"KRX ì—…ì¢…ëª…": "name", "ìƒì„¸ë‚´ìš©": "description"}
).to_dict(orient='records'),
            "similar_past_issues": top_past[['Issue_name', 'Contents', 'score']].rename(
    columns={"Issue_name": "title", "Contents": "content"}
).to_dict(orient='records')
        }

class RAGAnalyzer:
    """RAG ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •"""
        load_dotenv(override=True)
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        
        if not self.openai_api_key or not self.pinecone_api_key:
            raise ValueError("OPENAI_API_KEY ë˜ëŠ” PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o", 
            temperature=0, 
            max_tokens=4095,
            api_key=self.openai_api_key
        )
        
        self.embedding = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=self.openai_api_key
        )
        
        # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self._init_vector_stores()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._init_prompts()
    
    def _init_vector_stores(self):
        """Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            # ì‚°ì—… ë¶„ë¥˜ ë²¡í„° ìŠ¤í† ì–´
            self.industry_store = PineconeVectorStore(
                index_name="lastproject",
                embedding=self.embedding,
                namespace="industry"
            )
            
            # ê³¼ê±° ì´ìŠˆ ë²¡í„° ìŠ¤í† ì–´  
            self.past_issue_store = PineconeVectorStore(
                index_name="lastproject",
                embedding=self.embedding,
                namespace="past_issue"
            )
            
            print("âœ… Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _init_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        
        # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ ì£¼ì‹ ì‹œì¥ì— ëŒ€í•œ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ê°œì¸ íˆ¬ììë“¤ì´ ì‹œì¥ ë‰´ìŠ¤ë¥¼ ì´í•´í•˜ê³  í˜„ëª…í•œ íŒë‹¨ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- í˜„ì¬ ë‰´ìŠ¤ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³ ,
- ê³¼ê±° ìœ ì‚¬ ë‰´ìŠ¤ì™€ ë¹„êµí•´ ë³€í™”ëœ ì ì„ ë¶„ì„í•˜ë©°,  
- ê´€ë ¨ ì‚°ì—…ì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.

ì„¤ëª…ì€ íˆ¬ìì ì…ì¥ì—ì„œ ì•Œê¸° ì‰½ê²Œ í’€ì–´ì£¼ê³ , ì „ë¬¸ ìš©ì–´ëŠ” í”¼í•˜ë©°, ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í™œìš©í•´ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.
íˆ¬ì íŒë‹¨ì— ì°¸ê³ í•  ë§Œí•œ ì‹œì‚¬ì ì´ë‚˜ ì£¼ì˜í•  ì ë„ í¬í•¨í•´ ì£¼ì„¸ìš”.

**ì¤‘ìš”**: íˆ¬ì ì¶”ì²œì´ë‚˜ ë§¤ìˆ˜/ë§¤ë„ ê¶Œìœ ëŠ” ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. êµìœ¡ì  ë¶„ì„ë§Œ ì œê³µí•˜ì„¸ìš”.
            """),
            ("human", """
ì•„ë˜ ì„¸ ê°€ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.

## í˜„ì¬ ë‰´ìŠ¤:
{current_news}

## ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ:
{past_issues}

## ê´€ë ¨ ì‚°ì—… ì •ë³´:
{industry_context}

í˜•ì‹: 2~3ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.  
ë¶„ì„ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            """)
        ])
        
        # ì‹ ë¢°ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸
        self.confidence_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ ë¶„ì„ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ë‰´ìŠ¤ì™€ ê³¼ê±° ì´ìŠˆ, ì‚°ì—… ì •ë³´ ê°„ì˜ ê´€ë ¨ì„±ì„ 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 0.9~1.0: ë§¤ìš° ë†’ì€ ìœ ì‚¬ì„±, ëª…í™•í•œ ì—°ê´€ì„±
- 0.7~0.8: ë†’ì€ ìœ ì‚¬ì„±, ê°•í•œ ì—°ê´€ì„±  
- 0.5~0.6: ë³´í†µ ìœ ì‚¬ì„±, ì ë‹¹í•œ ì—°ê´€ì„±
- 0.3~0.4: ë‚®ì€ ìœ ì‚¬ì„±, ì•½í•œ ì—°ê´€ì„±
- 0.0~0.2: ë§¤ìš° ë‚®ì€ ìœ ì‚¬ì„±, ê±°ì˜ ë¬´ê´€

JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”: {"confidence": ì ìˆ˜, "reason": "í‰ê°€ ì´ìœ "}
            """),
            ("human", """
í˜„ì¬ ë‰´ìŠ¤: {current_news}
ê³¼ê±° ì´ìŠˆ: {past_issues}
ê´€ë ¨ ì‚°ì—…: {industry_context}
            """)
        ])
    
    async def search_similar_past_issues(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰: '{query[:50]}...'")
            
            # Pineconeì—ì„œ ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰
            similar_docs = self.past_issue_store.similarity_search(
                query, 
                k=top_k,
                namespace="past_issue"
            )
            
            results = []
            for i, doc in enumerate(similar_docs):
                # ë¬¸ì„œ ë‚´ìš© íŒŒì‹±
                content_lines = doc.page_content.split('\n')
                
                issue_data = {
                    "rank": i + 1,
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "parsed_info": self._parse_past_issue_content(content_lines)
                }
                
                results.append(issue_data)
            
            print(f"âœ… {len(results)}ê°œ ìœ ì‚¬ ì´ìŠˆ ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_related_industries(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰"""
        try:
            print(f"ğŸ­ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰: '{query[:50]}...'")
            
            # Pineconeì—ì„œ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰
            similar_docs = self.industry_store.similarity_search(
                query,
                k=top_k, 
                namespace="industry"
            )
            
            results = []
            for i, doc in enumerate(similar_docs):
                content_lines = doc.page_content.split('\n')
                
                industry_data = {
                    "rank": i + 1,
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "parsed_info": self._parse_industry_content(content_lines)
                }
                
                results.append(industry_data)
            
            print(f"âœ… {len(results)}ê°œ ê´€ë ¨ ì‚°ì—… ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_past_issue_content(self, content_lines: List[str]) -> Dict[str, str]:
        """ê³¼ê±° ì´ìŠˆ ë‚´ìš© íŒŒì‹±"""
        parsed = {
            "issue_name": "ë¯¸í™•ì¸",
            "related_industries": "ë¯¸í™•ì¸", 
            "industry_reason": "ë¯¸í™•ì¸",
            "start_date": "ë¯¸í™•ì¸",
            "end_date": "ë¯¸í™•ì¸"
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('Issue_name:'):
                parsed["issue_name"] = line.replace('Issue_name:', '').strip()
            elif line.startswith('ê´€ë ¨ ì‚°ì—…:'):
                parsed["related_industries"] = line.replace('ê´€ë ¨ ì‚°ì—…:', '').strip()
            elif line.startswith('ì‚°ì—… ì´ìœ :'):
                parsed["industry_reason"] = line.replace('ì‚°ì—… ì´ìœ :', '').strip()
            elif line.startswith('Start_date:'):
                parsed["start_date"] = line.replace('Start_date:', '').strip()
            elif line.startswith('Fin_date:'):
                parsed["end_date"] = line.replace('Fin_date:', '').strip()
        
        return parsed
    
    def _parse_industry_content(self, content_lines: List[str]) -> Dict[str, str]:
        """ì‚°ì—… ì •ë³´ ë‚´ìš© íŒŒì‹±"""
        parsed = {
            "industry_name": "ë¯¸í™•ì¸",
            "description": ""
        }
        
        for line in content_lines:
            line = line.strip()
            if line.startswith('KRX ì—…ì¢…ëª…:') or line.startswith('ï»¿KRX ì—…ì¢…ëª…:'):
                parsed["industry_name"] = line.replace('KRX ì—…ì¢…ëª…:', '').replace('ï»¿KRX ì—…ì¢…ëª…:', '').strip()
            elif line.startswith('ìƒì„¸ë‚´ìš©:'):
                parsed["description"] = line.replace('ìƒì„¸ë‚´ìš©:', '').strip()
            elif parsed["description"] and not line.startswith('KRX') and line:
                # ìƒì„¸ë‚´ìš© continuation
                parsed["description"] += " " + line
        
        return parsed
    
    async def comprehensive_analysis(self, current_news: str, max_past_issues: int = 3, max_industries: int = 3) -> Dict:
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        try:
            print("ğŸ§  ì¢…í•© ë¶„ì„ ì‹œì‘...")
            
            # 1. ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰
            print(f"ğŸ” ê³¼ê±° ìœ ì‚¬ ì´ìŠˆ ê²€ìƒ‰: '{current_news[:50]}...'")
            past_issues = self.search_past_issues(current_news, top_k=max_past_issues)
            print(f"âœ… {len(past_issues)}ê°œ ìœ ì‚¬ ì´ìŠˆ ë°œê²¬")
            
            # 2. ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰  
            print(f"ğŸ­ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰: '{current_news[:50]}...'")
            industries = self.search_industries(current_news, top_k=max_industries)
            print(f"âœ… {len(industries)}ê°œ ê´€ë ¨ ì‚°ì—… ë°œê²¬")
            
            # 3. LLM ì¬ë­í‚¹ ë¶€ë¶„ ì™„ì „ ì œê±° - ë°”ë¡œ ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©
            final_past_issues = past_issues[:max_past_issues]
            final_industries = industries[:max_industries]
            
            # 4. ìµœì¢… LLM ë¶„ì„
            print("ğŸ¯ ìµœì¢… ë¶„ì„ ìƒì„± ì¤‘...")
            explanation = self._generate_explanation(
                current_news, 
                final_past_issues, 
                final_industries
            )
            
            return {
                "explanation": explanation,
                "past_issues": final_past_issues,
                "industries": final_industries,
                "metadata": {
                    "analysis_time": datetime.now().isoformat(),
                    "past_issues_count": len(final_past_issues),
                    "industries_count": len(final_industries)
                }
            }
            
        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {
                "explanation": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "past_issues": [],
                "industries": [],
                "metadata": {"error": str(e)}
            }
    
    def _format_past_issues_for_analysis(self, past_issues: List[Dict]) -> str:
        if not past_issues:
            return "ê´€ë ¨ëœ ê³¼ê±° ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_texts = []
        for i, issue in enumerate(past_issues[:3]):
            parsed = issue.get("parsed_info", {})
            reason = issue.get("reason", "LLM íŒë‹¨ ì´ìœ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            text = f"""
    [ê³¼ê±° ì´ìŠˆ {i+1}]
    â€¢ ì´ìŠˆëª…: {parsed.get('issue_name', 'ë¯¸í™•ì¸')}
    â€¢ ê´€ë ¨ ì‚°ì—…: {parsed.get('related_industries', 'ë¯¸í™•ì¸')}
    â€¢ ë°œìƒ ê¸°ê°„: {parsed.get('start_date', 'ë¯¸í™•ì¸')} ~ {parsed.get('end_date', 'ë¯¸í™•ì¸')}
    â€¢ ì„ íƒëœ ì´ìœ  (LLM): {reason}
            """.strip()
            
            formatted_texts.append(text)
        
        return "\n\n".join(formatted_texts)
    
    def _format_industries_for_analysis(self, industries: List[Dict]) -> str:
        if not industries:
            return "ê´€ë ¨ëœ ì‚°ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        formatted_texts = []
        for i, industry in enumerate(industries[:3]):
            parsed = industry.get("parsed_info", {})
            reason = industry.get("reason", "LLM íŒë‹¨ ì´ìœ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            text = f"""
    [ê´€ë ¨ ì‚°ì—… {i+1}]
    â€¢ ì‚°ì—…ëª…: {parsed.get('industry_name', 'ë¯¸í™•ì¸')}
    â€¢ ì„ íƒëœ ì´ìœ  (LLM): {reason}
            """.strip()

            formatted_texts.append(text)

        return "\n\n".join(formatted_texts)
    
    async def quick_analysis(self, news_content: str) -> Dict[str, Any]:
        """ë¹ ë¥¸ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            # ê¸°ë³¸ ì‚°ì—… ë§¤ì¹­ë§Œ ìˆ˜í–‰
            industries = await self.search_related_industries(news_content, top_k=2)
            
            if not industries:
                return {
                    "explanation": "ê´€ë ¨ ì‚°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "confidence": 0.0,
                    "industries": []
                }
            
            # ê°„ë‹¨í•œ ì„¤ëª… ìƒì„±
            industry_names = [ind.get("parsed_info", {}).get("industry_name", "ë¯¸í™•ì¸") 
                            for ind in industries]
            
            explanation = f"""
í˜„ì¬ ë‰´ìŠ¤ëŠ” ì£¼ë¡œ {', '.join(industry_names)} ì‚°ì—…ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì‚°ì—…ë“¤ì€ í•´ë‹¹ ë‰´ìŠ¤ì˜ ì˜í–¥ì„ ì§ê°„ì ‘ì ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, 
íˆ¬ììë“¤ì€ ì´ëŸ¬í•œ ì‚°ì—… ë™í–¥ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ë” ìƒì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ ì „ì²´ ë¶„ì„ ê¸°ëŠ¥ì„ ì´ìš©í•´ì£¼ì„¸ìš”.
            """.strip()
            
            return {
                "explanation": explanation,
                "confidence": 0.6,
                "industries": industry_names
            }
            
        except Exception as e:
            print(f"âŒ ë¹ ë¥¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "explanation": "ë¹ ë¥¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "confidence": 0.0,
                "industries": []
            }

# ===== í…ŒìŠ¤íŠ¸ ë° ì§ì ‘ ì‹¤í–‰ìš© =====

async def test_rag_system():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª RAG ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    try:
        # RAG ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = RAGAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤
        test_news = """
SKí…”ë ˆì½¤(SKT)ì´ ìµœê·¼ ì‚¬ì´ë²„ ë³´ì•ˆ ì‚¬ê³ ë¡œ ì¸í•´ ê³ ê°ë“¤ì—ê²Œ ê³„ì•½ í•´ì§€ ìˆ˜ìˆ˜ë£Œë¥¼ ë©´ì œí•´ ì£¼ê² ë‹¤ê³  ë°œí‘œí–ˆëŠ”ë°, 
ì´ë¡œ ì¸í•´ ë§ì€ ê³ ê°ë“¤ì´ ê²½ìŸì‚¬ì¸ KTì™€ LG U+ë¡œ ì´ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
í•˜ë£¨ì—ë§Œ 6,656ëª…ì˜ ìˆœì†ì‹¤ì„ ê¸°ë¡í–ˆìœ¼ë©°, ì¼ì£¼ì¼ ë™ì•ˆ 28,566ëª…ì˜ ìˆœì†ì‹¤ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
        """
        
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤: {test_news[:100]}...")
        
        # 1. ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ê³¼ê±° ì´ìŠˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        past_issues = await analyzer.search_similar_past_issues(test_news)
        for issue in past_issues:
            parsed = issue["parsed_info"]
            print(f"  â€¢ {parsed['issue_name']}")
        
        # 2. ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
        print("\n2ï¸âƒ£ ê´€ë ¨ ì‚°ì—… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        industries = await analyzer.search_related_industries(test_news)
        for industry in industries:
            parsed = industry["parsed_info"]
            print(f"  â€¢ {parsed['industry_name']}")
        
        # 3. ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸")
        analysis = await analyzer.comprehensive_analysis(test_news)
        print(f"ì‹ ë¢°ë„: {analysis['confidence']:.2f}")
        print(f"ë¶„ì„ ê²°ê³¼:\n{analysis['explanation'][:200]}...")
        
        # 4. ë¹ ë¥¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ ë¹ ë¥¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        quick = await analyzer.quick_analysis(test_news)
        print(f"ê´€ë ¨ ì‚°ì—…: {quick['industries']}")
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

from scripts.vector_search import VectorSearchEngine
from langchain_openai import OpenAIEmbeddings

class EmbeddingBasedAnalyzer:
    """OpenAI ì„ë² ë”© ê¸°ë°˜ ì‚°ì—…/ê³¼ê±°ì´ìŠˆ ìœ ì‚¬ë„ ë¶„ì„ê¸°"""

    def __init__(self):
        self.vector_engine = VectorSearchEngine()
        self.embedding_model = OpenAIEmbeddings()

    def analyze(self, content: str, top_k: int = 5) -> Dict[str, Any]:
        # í…ìŠ¤íŠ¸ â†’ ì„ë² ë”© ë²¡í„°í™”
        embedding = self.embedding_model.embed_query(content)

        # ì‚°ì—…/ì´ìŠˆ ê²€ìƒ‰
        industry_matches = self.vector_engine.query_embedding("industry", embedding, top_k)
        issue_matches = self.vector_engine.query_embedding("past_issue", embedding, top_k)

        # ê²°ê³¼ í¬ë§· ì •ì œ
        related_industries = [
            {
                "name": item["metadata"].get("name", ""),
                "description": item["metadata"].get("description", ""),
                "score": item.get("score", 0.0)
            }
            for item in industry_matches
        ]
        similar_past_issues = [
            {
                "title": item["metadata"].get("title", ""),
                "content": item["metadata"].get("content", ""),
                "score": item.get("score", 0.0)
            }
            for item in issue_matches
        ]

        return {
            "related_industries": related_industries,
            "similar_past_issues": similar_past_issues
                    }

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_rag_system())

