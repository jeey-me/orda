#!/usr/bin/env python3
"""
Stock Market Relevance Filter
70ê°œ ì´ìŠˆ ì¤‘ ì£¼ì‹ì‹œì¥ê³¼ ê°€ì¥ ë°€ì ‘í•œ 5ê°œ ì´ìŠˆë¥¼ AIë¡œ ì„ ë³„
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

class StockMarketFilter:
    """ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ê¸°ë°˜ ì´ìŠˆ í•„í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, model: str = "gpt-4o-mini", temperature: float = 0.1):
        """
        í•„í„° ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  OpenAI ëª¨ë¸
            temperature: ëª¨ë¸ temperature ì„¤ì •
        """
        self.llm = ChatOpenAI(model=model, temperature=temperature)
        self.filter_prompt = self._create_filter_prompt()
        self.detailed_analysis_prompt = self._create_detailed_analysis_prompt()
        
        print(f"ğŸ¤– StockMarketFilter ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model})")

    def _create_filter_prompt(self) -> ChatPromptTemplate:
        """ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± í•„í„°ë§ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” í•œêµ­ ì£¼ì‹ì‹œì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼. 
ì£¼ì–´ì§„ ë‰´ìŠ¤ ì´ìŠˆë“¤ì„ ë¶„ì„í•˜ì—¬ ì£¼ì‹ì‹œì¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì´ìŠˆë“¤ì„ ì„ ë³„í•´ì•¼ í•´.

ğŸ“Š í‰ê°€ ê¸°ì¤€ (ê° 1-10ì ):
1. **ì§ì ‘ì  ê¸°ì—… ì˜í–¥**: íŠ¹ì • ê¸°ì—…ì´ë‚˜ ì‚°ì—…ì˜ ì‹¤ì ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
2. **ì •ì±…ì  ì˜í–¥**: ê¸ˆë¦¬, ì„¸ê¸ˆ, ê·œì œ ë³€í™” ë“± ì‹œì¥ ì „ë°˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì •ì±…ì¸ê°€?
3. **ì‹œì¥ ì‹¬ë¦¬**: íˆ¬ìì ì‹ ë¢°ë„, ë¦¬ìŠ¤í¬ ì¸ì‹, íˆ¬ì ì‹¬ë¦¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?
4. **ê±°ì‹œê²½ì œ**: GDP, ì¸í”Œë ˆì´ì…˜, í™˜ìœ¨ ë“± ê±°ì‹œê²½ì œ ì§€í‘œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?
5. **ì‚°ì—… íŠ¸ë Œë“œ**: ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ì†Œë¹„ íŒ¨í„´ ë³€í™”ë¡œ ì¸í•œ ì‚°ì—… ì˜í–¥ì€?

ğŸ’¡ ìš°ì„ ìˆœìœ„:
- ë‹¨ê¸°ì  ì£¼ê°€ ë³€ë™ì„ ì¼ìœ¼í‚¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ìŠˆ
- íŠ¹ì • ì—…ì¢…ì´ë‚˜ í…Œë§ˆì£¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì´ìŠˆ
- ì™¸êµ­ì¸ íˆ¬ìë‚˜ ê¸°ê´€ íˆ¬ìì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì´ìŠˆ
- ì •ë¶€ ì •ì±…ì´ë‚˜ ê·œì œ ë³€í™” ê´€ë ¨ ì´ìŠˆ"""),
            
            ("human", """ë‹¤ìŒ {total_count}ê°œì˜ ë‰´ìŠ¤ ì´ìŠˆ ì¤‘ì—ì„œ ì£¼ì‹ì‹œì¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ìƒìœ„ 5ê°œë¥¼ ì„ ë³„í•´ì£¼ì„¸ìš”.

ğŸ“° ì´ìŠˆ ëª©ë¡:
{issues_list}

ê° ì´ìŠˆì— ëŒ€í•´ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ê³ , ìƒìœ„ 5ê°œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:
- ì¢…í•© ì ìˆ˜ (1-10ì )
- ì£¼ëœ ì˜í–¥ ë¶„ì•¼ (ì˜ˆ: ê¸ˆìœµì£¼, ë°”ì´ì˜¤ì£¼, ë°˜ë„ì²´ ë“±)
- ì˜ˆìƒ ì˜í–¥ ë°©í–¥ (ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì )
- ì˜í–¥ ì‹œê¸° (ì¦‰ì‹œ/ë‹¨ê¸°/ì¤‘ê¸°)

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "analysis_summary": {{
    "total_analyzed": {total_count},
    "selected_count": 5,
    "analysis_timestamp": "í˜„ì¬ ì‹œê°„",
    "filter_confidence": "ë¶„ì„ ì‹ ë¢°ë„ (1-10ì )"
  }},
  "selected_issues": [
    {{
      "rank": 1,
      "ì´ìŠˆë²ˆí˜¸": ì›ë³¸_ì´ìŠˆë²ˆí˜¸,
      "ì¹´í…Œê³ ë¦¬": "ì›ë³¸_ì¹´í…Œê³ ë¦¬",
      "ì œëª©": "ì›ë³¸_ì œëª©",
      "ì¢…í•©ì ìˆ˜": 9.2,
      "ì§ì ‘ê¸°ì—…ì˜í–¥": 9,
      "ì •ì±…ì˜í–¥": 8,
      "ì‹œì¥ì‹¬ë¦¬": 9,
      "ê±°ì‹œê²½ì œ": 7,
      "ì‚°ì—…íŠ¸ë Œë“œ": 8,
      "ì£¼ëœì˜í–¥ë¶„ì•¼": ["ê¸ˆìœµì£¼", "ì€í–‰ì£¼"],
      "ì˜ˆìƒì˜í–¥ë°©í–¥": "ê¸ì •ì ",
      "ì˜í–¥ì‹œê¸°": "ì¦‰ì‹œ",
      "ì„ ë³„ì´ìœ ": "êµ¬ì²´ì ì¸ ì„ ë³„ ì´ìœ  ì„¤ëª…",
      "ì˜ˆìƒì‹œì¥ë°˜ì‘": "ì˜ˆìƒë˜ëŠ” ì‹œì¥ ë°˜ì‘ ì„¤ëª…"
    }},
    ...
  ],
  "filtering_notes": "ì „ë°˜ì ì¸ ì„ ë³„ ê³¼ì •ì—ì„œì˜ íŠ¹ì´ì‚¬í•­ì´ë‚˜ ê³ ë ¤ì‚¬í•­"
}}""")
        ])

    def _create_detailed_analysis_prompt(self) -> ChatPromptTemplate:
        """ì„ ë³„ëœ ì´ìŠˆë“¤ì˜ ìƒì„¸ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸"""
        return ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” í•œêµ­ ì£¼ì‹ì‹œì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼. 
ì„ ë³„ëœ ì£¼ìš” ì´ìŠˆë“¤ì— ëŒ€í•´ ë” ìì„¸í•œ ì‹œì¥ ì˜í–¥ ë¶„ì„ì„ ì œê³µí•´ì•¼ í•´."""),
            
            ("human", """ë‹¤ìŒ 5ê°œ ì´ìŠˆì— ëŒ€í•´ ìƒì„¸í•œ ì£¼ì‹ì‹œì¥ ì˜í–¥ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{selected_issues}

ê° ì´ìŠˆë³„ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ì˜í–¥ ìš”ì¸
2. ì˜í–¥ë°›ì„ êµ¬ì²´ì ì¸ ì¢…ëª©/ì„¹í„°
3. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ì™€ì˜ ë¹„êµ
4. íˆ¬ì ì „ëµ ì œì•ˆ
5. ë¦¬ìŠ¤í¬ ìš”ì¸

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "detailed_analysis": [
    {{
      "rank": 1,
      "ì œëª©": "ì´ìŠˆ ì œëª©",
      "í•µì‹¬ì˜í–¥ìš”ì¸": ["ìš”ì¸1", "ìš”ì¸2"],
      "ì˜í–¥ì„¹í„°": [
        {{"ì„¹í„°ëª…": "ê¸ˆìœµ", "ì˜í–¥ë„": "ë†’ìŒ", "ë°©í–¥": "ê¸ì •ì "}},
        {{"ì„¹í„°ëª…": "ë¶€ë™ì‚°", "ì˜í–¥ë„": "ì¤‘ê°„", "ë°©í–¥": "ë¶€ì •ì "}}
      ],
      "ê´€ë ¨ì¢…ëª©ì˜ˆì‹œ": ["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤"],
      "ê³¼ê±°ìœ ì‚¬ì‚¬ë¡€": "2020ë…„ ì½”ë¡œë‚˜19 ê¸ˆë¦¬ ì¸í•˜ ì‹œ ê¸ˆìœµì£¼ ê¸‰ë“±",
      "íˆ¬ìì „ëµ": "ë‹¨ê¸°ì ìœ¼ë¡œ ê¸ˆìœµì£¼ ë§¤ìˆ˜ í¬ì§€ì…˜ ê³ ë ¤",
      "ë¦¬ìŠ¤í¬ìš”ì¸": ["ì •ì±… ë³€í™” ê°€ëŠ¥ì„±", "ì™¸ë¶€ ê²½ì œ ë³€ìˆ˜"],
      "ì‹ ë¢°ë„": 8.5
    }}
  ],
  "market_outlook": {{
    "overall_sentiment": "ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì ",
    "key_themes": ["ì£¼ìš” í…Œë§ˆ1", "ì£¼ìš” í…Œë§ˆ2"],
    "attention_sectors": ["ì£¼ëª©í•  ì„¹í„°ë“¤"],
    "risk_factors": ["ì „ë°˜ì  ë¦¬ìŠ¤í¬ ìš”ì¸ë“¤"]
  }}
}}""")
        ])

    def filter_issues_by_stock_relevance(self, issues_data: List[Dict], target_count: int = 5) -> Dict:
        """
        ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ìŠˆ í•„í„°ë§
        
        Args:
            issues_data: í¬ë¡¤ë§ëœ ì „ì²´ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
            target_count: ì„ ë³„í•  ì´ìŠˆ ìˆ˜
            
        Returns:
            í•„í„°ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not issues_data:
            raise ValueError("í•„í„°ë§í•  ì´ìŠˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ” ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± í•„í„°ë§ ì‹œì‘")
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {len(issues_data)}ê°œ ì´ìŠˆ")
        print(f"ğŸ¯ ì„ ë³„ ëª©í‘œ: {target_count}ê°œ ì´ìŠˆ")
        
        try:
            # 1. ì´ìŠˆ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            issues_text = self._format_issues_for_analysis(issues_data)
            
            # 2. AI í•„í„°ë§ ì‹¤í–‰
            print("ğŸ¤– AI ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            filtering_result = self._execute_filtering(issues_text, len(issues_data))
            
            # 3. ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„
            validated_result = self._validate_filtering_result(filtering_result, issues_data)
            
            # 4. ìƒì„¸ ë¶„ì„ ì¶”ê°€
            print("ğŸ“ˆ ìƒì„¸ ì‹œì¥ ì˜í–¥ ë¶„ì„ ì¤‘...")
            detailed_analysis = self._generate_detailed_analysis(validated_result["selected_issues"])
            
            # 5. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "filter_metadata": {
                    "filtered_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "total_analyzed": len(issues_data),
                    "selected_count": len(validated_result["selected_issues"]),
                    "filter_model": "gpt-4o",
                    "filter_confidence": validated_result.get("analysis_summary", {}).get("filter_confidence", "N/A")
                },
                "selected_issues": validated_result["selected_issues"],
                "detailed_analysis": detailed_analysis.get("detailed_analysis", []),
                "market_outlook": detailed_analysis.get("market_outlook", {}),
                "filtering_notes": validated_result.get("filtering_notes", ""),
                "original_issues_count": len(issues_data)
            }
            
            print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(final_result['selected_issues'])}ê°œ ì´ìŠˆ ì„ ë³„")
            self._print_filtering_summary(final_result)
            
            return final_result
            
        except Exception as e:
            print(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            raise

    def _format_issues_for_analysis(self, issues_data: List[Dict]) -> str:
        """ì´ìŠˆ ë°ì´í„°ë¥¼ AI ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        formatted_issues = []
        
        for issue in issues_data:
            issue_text = f"""
ì´ìŠˆë²ˆí˜¸: {issue.get('ì´ìŠˆë²ˆí˜¸', 'N/A')}
ì¹´í…Œê³ ë¦¬: {issue.get('ì¹´í…Œê³ ë¦¬', 'N/A')}
ì œëª©: {issue.get('ì œëª©', 'N/A')}
ë‚´ìš©: {issue.get('ë‚´ìš©', 'N/A')[:200]}...
"""
            formatted_issues.append(issue_text.strip())
        
        return "\n" + "="*50 + "\n".join(formatted_issues)

    def _execute_filtering(self, issues_text: str, total_count: int) -> Dict:
        """AI í•„í„°ë§ ì‹¤í–‰"""
        try:
            parser = JsonOutputParser()
            chain = self.filter_prompt | self.llm | parser
            
            result = chain.invoke({
                "issues_list": issues_text,
                "total_count": total_count
            })
            
            return result
            
        except Exception as e:
            print(f"âŒ AI í•„í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise

    def _validate_filtering_result(self, filtering_result: Dict, original_issues: List[Dict]) -> Dict:
        """í•„í„°ë§ ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„"""
        try:
            selected_issues = filtering_result.get("selected_issues", [])
            
            # ì„ ë³„ëœ ì´ìŠˆì— ì›ë³¸ ë°ì´í„° ë³´ì™„
            for selected in selected_issues:
                issue_num = selected.get("ì´ìŠˆë²ˆí˜¸")
                original_issue = next((issue for issue in original_issues if issue.get("ì´ìŠˆë²ˆí˜¸") == issue_num), None)
                
                if original_issue:
                    # ì›ë³¸ ë°ì´í„°ë¡œ ë³´ì™„
                    selected["ì›ë³¸ë‚´ìš©"] = original_issue.get("ë‚´ìš©", "")
                    selected["ì¶”ì¶œì‹œê°„"] = original_issue.get("ì¶”ì¶œì‹œê°„", "")
                    selected["ê³ ìœ ID"] = original_issue.get("ê³ ìœ ID", "")
                else:
                    print(f"âš ï¸ ì´ìŠˆë²ˆí˜¸ {issue_num}ì— ëŒ€í•œ ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìˆœìœ„ ê²€ì¦ ë° ì •ë ¬
            selected_issues.sort(key=lambda x: x.get("rank", 999))
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            for i, issue in enumerate(selected_issues, 1):
                if "rank" not in issue:
                    issue["rank"] = i
                if "ì¢…í•©ì ìˆ˜" not in issue:
                    issue["ì¢…í•©ì ìˆ˜"] = 7.0  # ê¸°ë³¸ê°’
            
            filtering_result["selected_issues"] = selected_issues
            return filtering_result
            
        except Exception as e:
            print(f"âš ï¸ í•„í„°ë§ ê²°ê³¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return filtering_result

    def _generate_detailed_analysis(self, selected_issues: List[Dict]) -> Dict:
        """ì„ ë³„ëœ ì´ìŠˆë“¤ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ ìƒì„±"""
        try:
            # ìƒì„¸ ë¶„ì„ìš© ë°ì´í„° í¬ë§·íŒ…
            issues_summary = []
            for issue in selected_issues:
                summary = f"""
ìˆœìœ„ {issue.get('rank', 'N/A')}: {issue.get('ì œëª©', 'N/A')}
ì¹´í…Œê³ ë¦¬: {issue.get('ì¹´í…Œê³ ë¦¬', 'N/A')}
ì¢…í•©ì ìˆ˜: {issue.get('ì¢…í•©ì ìˆ˜', 'N/A')}
ì„ ë³„ì´ìœ : {issue.get('ì„ ë³„ì´ìœ ', 'N/A')}
"""
                issues_summary.append(summary.strip())
            
            issues_text = "\n" + "="*30 + "\n".join(issues_summary)
            
            parser = JsonOutputParser()
            chain = self.detailed_analysis_prompt | self.llm | parser
            
            result = chain.invoke({
                "selected_issues": issues_text
            })
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ ìƒì„¸ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"detailed_analysis": [], "market_outlook": {}}

    def _print_filtering_summary(self, result: Dict):
        """í•„í„°ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± í•„í„°ë§ ê²°ê³¼")
        print(f"{'='*80}")
        
        metadata = result["filter_metadata"]
        print(f"ğŸ• í•„í„°ë§ ì‹œê°„: {metadata['filtered_at']}")
        print(f"ğŸ“ˆ ë¶„ì„ ëª¨ë¸: {metadata['filter_model']}")
        print(f"ğŸ¯ ì„ ë³„ ë¹„ìœ¨: {metadata['selected_count']}/{metadata['total_analyzed']} ({metadata['selected_count']/metadata['total_analyzed']*100:.1f}%)")
        print(f"ğŸ’¯ í•„í„° ì‹ ë¢°ë„: {metadata['filter_confidence']}")
        
        print(f"\nğŸ† ì„ ë³„ëœ ìƒìœ„ {len(result['selected_issues'])}ê°œ ì´ìŠˆ:")
        for issue in result["selected_issues"]:
            print(f"   {issue['rank']}. [{issue['ì¹´í…Œê³ ë¦¬']}] {issue['ì œëª©'][:50]}...")
            print(f"      ğŸ’° ì¢…í•©ì ìˆ˜: {issue['ì¢…í•©ì ìˆ˜']}/10")
            print(f"      ğŸ“Š ì£¼ëœì˜í–¥: {', '.join(issue.get('ì£¼ëœì˜í–¥ë¶„ì•¼', ['N/A']))}")
            print(f"      â° ì˜í–¥ì‹œê¸°: {issue.get('ì˜í–¥ì‹œê¸°', 'N/A')}")
            print()
        
        # ì‹œì¥ ì „ë§ ìš”ì•½
        outlook = result.get("market_outlook", {})
        if outlook:
            print(f"ğŸ”® ì „ë°˜ì  ì‹œì¥ ì „ë§:")
            print(f"   â€¢ ì‹œì¥ ì‹¬ë¦¬: {outlook.get('overall_sentiment', 'N/A')}")
            print(f"   â€¢ ì£¼ìš” í…Œë§ˆ: {', '.join(outlook.get('key_themes', []))}")
            print(f"   â€¢ ì£¼ëª© ì„¹í„°: {', '.join(outlook.get('attention_sectors', []))}")

    def save_filtered_results(self, results: Dict, data_dir: str = "data2") -> str:
        """í•„í„°ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            data_path = Path(data_dir)
            data_path.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_StockFiltered_5issues.json"
            filepath = data_path / filename
            
            save_data = {
                **results,
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "filter_version": "StockMarketFilter_v1.0"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í•„í„°ë§ ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_latest_filtered_results(self, data_dir: str = "data2") -> Optional[Dict]:
        """ìµœì‹  í•„í„°ë§ ê²°ê³¼ ë¡œë“œ"""
        try:
            data_path = Path(data_dir)
            json_files = list(data_path.glob("*_StockFiltered_*issues.json"))
            
            if not json_files:
                print("ğŸ“‚ ì €ì¥ëœ í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… ìµœì‹  í•„í„°ë§ ê²°ê³¼ ë¡œë“œ: {latest_file.name}")
            return data
            
        except Exception as e:
            print(f"âŒ í•„í„°ë§ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

# í¸ì˜ í•¨ìˆ˜ë“¤
def filter_issues_for_stock_market(issues_data: List[Dict], target_count: int = 5) -> Dict:
    """ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ê¸°ì¤€ ì´ìŠˆ í•„í„°ë§ í¸ì˜ í•¨ìˆ˜"""
    filter_system = StockMarketFilter()
    return filter_system.filter_issues_by_stock_relevance(issues_data, target_count)

def load_and_filter_latest_crawled_data(data_dir: str = "data2") -> Optional[Dict]:
    """ìµœì‹  í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ í•„í„°ë§"""
    from crawling_bigkinds import BigKindsCrawler  # ë³€ê²½ëœ ë¶€ë¶„
    
    # ìµœì‹  í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ
    crawled_data = load_latest_multi_data()
    if not crawled_data or not crawled_data.get("all_issues"):
        print("âŒ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # í•„í„°ë§ ì‹¤í–‰
    filter_system = StockMarketFilter()
    filtered_result = filter_system.filter_issues_by_stock_relevance(crawled_data["all_issues"])
    
    # ê²°ê³¼ ì €ì¥
    saved_file = filter_system.save_filtered_results(filtered_result, data_dir)
    filtered_result["saved_file"] = saved_file
    
    return filtered_result

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ¤– Stock Market Relevance Filter")
    
    try:
        # ìµœì‹  í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
        print("ğŸ“¥ ìµœì‹  í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ ì¤‘...")
        result = load_and_filter_latest_crawled_data()
        
        if result:
            print(f"\nğŸ¯ í•„í„°ë§ ì™„ë£Œ!")
            print(f"   â€¢ ì €ì¥ íŒŒì¼: {result.get('saved_file', 'N/A')}")
            print(f"   â€¢ ì„ ë³„ëœ ì´ìŠˆ ìˆ˜: {len(result.get('selected_issues', []))}ê°œ")
            
            # ì„ ë³„ëœ ì´ìŠˆ ê°„ë‹¨ ë¯¸ë¦¬ë³´ê¸°
            selected = result.get("selected_issues", [])
            if selected:
                print(f"\nğŸ† TOP 3 ì´ìŠˆ:")
                for issue in selected[:3]:
                    print(f"   {issue['rank']}. {issue['ì œëª©'][:50]}... (ì ìˆ˜: {issue['ì¢…í•©ì ìˆ˜']})")
        else:
            print("âŒ í•„í„°ë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ í•„í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()